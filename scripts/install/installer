#!/usr/bin/env python3
from collections import namedtuple
import netaddr
import copy
import sys
import click
import os
import urllib
import requests
import random
import yaml
import json
import time
import jsonschema
import concurrent.futures.thread
import logging
import subprocess
from datetime import datetime
from baldur import remote, network, teleport, kubernetes, etcdcluster
import threading
from functools import partial
import string


BASE_IMAGE_EXCLUDE = [
    "dev",
    "sys",
    "proc",
    "etc/resolv.conf",
    "etc/hosts",
    "etc/hostname",
    "etc/mtab",
]

Version = namedtuple("Version", ("version", "type"))
REPO_URL = "https://github.com/0-complexity/openvcloud_installer"
DEV_REPOS = [
    "https://github.com/0-complexity/openvcloud",
    "https://github.com/0-complexity/selfhealing",
    "https://github.com/jumpscale7/jumpscale_core7",
    "https://github.com/jumpscale7/jumpscale_portal",
]
REPO_PATH = "/opt/code/github/0-complexity/openvcloud_installer"
DEV_TMPLS = ["osis", "agentcontroller", "portal"]
REPOREVISIONS = {}
BASE_FS_PATH = "/var/ovc/base_images/"

ERROR_COLOR = u"\u001b[31m"
RESET_COLOR = u"\u001b[0m"
COLORBLUE = u"\u001b[34m"
COLORGREEN = u"\u001b[32m"
CURSORUP = u"\u001b[{n}A"
CLEAREOL = u"\u001b[0K"
ISINTERACTIVE = sys.stdout.isatty()

status = {}
lock = threading.Lock()
logging.raiseExceptions = False
logging.getLogger("paramiko").setLevel(logging.ERROR)


def log(msg):
    sys.stdout.write(msg + "\n")
    sys.stdout.flush()


def print_status(clear=False):
    with lock:
        if not ISINTERACTIVE:
            return
        if clear:
            sys.stdout.write(CURSORUP.format(n=len(status)))
        length = len(max(status.keys(), key=lambda x: len(x)))
        for node, line in sorted(status.items()):
            line = "{blue}{node:{length}}{reset}: {msg}{eol}".format(
                blue=COLORBLUE,
                node=node,
                length=length,
                reset=RESET_COLOR,
                msg=line,
                eol=CLEAREOL,
            )
            log(line)


def get_revision(repo):
    version = version_type(repo)
    match = "tags/" if version.type == "tag" else "heads/"
    match += version.version
    output = local_execute(["git", "ls-remote", repo["url"]])
    for line in output.splitlines():
        if line.endswith(match):
            return line.split()[0]


def version_type(repo):
    for vtype in ("tag", "branch"):
        if vtype in repo["target"]:
            return Version(repo["target"][vtype], vtype)
    raise RuntimeError("No tag or branch set on repo")


def update_repo_volume(versions, repo):
    url = repo["repository"]
    if url in REPOREVISIONS:
        repo["revision"] = REPOREVISIONS[url]
        return
    for vrepo in versions["repos"]:
        if vrepo["url"] == url:
            revision = get_revision(vrepo)
            if revision:
                repo["revision"] = revision
                REPOREVISIONS[url] = revision
            return


def get_ipaddress(net):
    return str(netaddr.IPNetwork(net).ip)


def get_pnodes(
    config,
    versions=None,
    roles=None,
    enabled_only=False,
    active_only=False,
    fs_path=None,
):
    pnodes = []
    roles = set(roles or [])
    nodes = config["nodes"]

    if enabled_only:
        enabled_pnodes = [
            name
            for name, status in get_nodes_status(config).items()
            if status == "ENABLED"
        ]
        nodes = [node for node in config["nodes"] if node["name"] in enabled_pnodes]

    for pnode in nodes:
        if roles.intersection(set(pnode["roles"])):
            node = PNode(config, pnode["name"], versions, fs_path)
            if not active_only or node.is_up():
                pnodes.append(node)
    return pnodes


def local_execute(*args, **kwargs):
    kwargs["check"] = kwargs.get("check", True)
    kwargs["stdout"] = kwargs.get("stdout", subprocess.PIPE)
    result = subprocess.run(*args, **kwargs).stdout.decode("utf-8")
    return result


def set_cgroup(node, cgroup, cpu_set, memory):
    mem_path = "/sys/fs/cgroup/memory/{}".format(cgroup)
    cpu_path = "/sys/fs/cgroup/cpuset/{}".format(cgroup)
    command = """
    set -e
    mkdir -p {mem}
    mkdir -p {cpu}
    """.format(
        mem=mem_path, cpu=cpu_path
    )
    node.run(command)
    node.write_file("{}/cpuset.cpus".format(cpu_path), str(cpu_set))
    node.write_file("{}/memory.limit_in_bytes".format(mem_path), str(memory))


def portal_api_call(endpoint, config, params=None, data=None):
    url = "http://portal:82/restmachine/%s" % endpoint
    response = requests.post(
        url,
        data=data,
        params=params,
        headers={
            "Authorization": "authkey {0}".format(config["environment"]["password"])
        },
    )
    return response


def get_nodes_status(config):
    status_dict = dict()
    response = portal_api_call("system/gridmanager/getNodes", config)
    response.raise_for_status()
    for node in response.json():
        status_dict[node["name"].split(".")[0]] = node["status"]
    return status_dict


def pnode_action(pnodes, action):
    futures = {}
    running_futures = {}
    results = ""
    with concurrent.futures.thread.ThreadPoolExecutor(len(pnodes)) as proc:
        for pnode in pnodes:
            status[pnode.name] = "Started {}".format(action)

        print("Waiting for {action} node to be done...".format(action=action))
        print_status()
        for pnode in pnodes:
            func = getattr(pnode, action)
            fut = proc.submit(func)
            futures[pnode.name] = fut
            running_futures[pnode.name] = fut

        def update_status():
            messages = ""
            future_error = None
            for name, future in list(running_futures.items()):
                if future.done():
                    if future.exception():
                        messages += "{color}Error when performing node {action} {name}: {reset}{exc} \n".format(
                            color=ERROR_COLOR,
                            action=action,
                            name=name,
                            reset=RESET_COLOR,
                            exc=str(future.exception()),
                        )
                        future_error = future.exception()
                        status[name] = "{}ERROR: details will follow{}".format(
                            ERROR_COLOR, RESET_COLOR
                        )
                        print_status(True)
                    else:
                        status[name] = "{}Finished{}".format(COLORGREEN, RESET_COLOR)
                        print_status(True)
                    running_futures.pop(name)
            return future_error, messages

        haserrors = False
        while running_futures:
            time.sleep(5)
            error, msg = update_status()
            if error is not None:
                haserrors = True
            results += msg
        log(results)
        if haserrors:
            raise RuntimeError("Failed to apply {}".format(action))


def is_conv_env(config):
    nodes = config["nodes"]
    roles = {"controller", "storage", "cpu"}
    for node in nodes:
        roles_intersection = roles.intersection(set(node["roles"]))
        if len(roles_intersection) > 1 and "controller" in node["roles"]:
            return True
    return False


def prepare_config(config_path):
    def _helper(nodes):
        for node in nodes:
            net = netaddr.IPNetwork(value["network"])
            ip = net.ip + node["ip-lsb"]
            if key not in node:
                node[key] = {}
            node[key]["ipaddress"] = "{ip}/{sub}".format(ip=ip, sub=net.prefixlen)

    with open(config_path, "r") as cfg:
        config = yaml.load(cfg)
    # add default directories for syncthing
    default_dirs = [
        {"path": "/var/ovc/billing", "sync": True},
        {"path": "/var/ovc/influxdb", "sync": True},
        {"path": "/var/ovc/mongodb", "sync": False},
        {"path": "/var/ovc/pxeboot", "sync": True},
        {"path": "/var/ovc/0-access-hostkeys", "sync": True},
        {"path": "/var/ovc/grafana", "sync": True},
        {"path": "/var/ovc/0-access", "sync": True},
        {"path": "/var/ovc/ssl", "sync": True},
        {"path": "/var/ovc/.ssh", "sync": True, "chmod": 0o700},
        {"path": "/var/ovc/updatelogs", "sync": True},
    ]
    dirpaths = set(dir["path"] for dir in default_dirs)
    for dir in config.get("directories", []):
        if dir["path"] not in dirpaths:
            default_dirs.append(dir)
            dirpaths.add(dir["path"])
    config["directories"] = default_dirs
    with open(
        "{}/scripts/kubernetes/config/config-validator.json".format(REPO_PATH), "r"
    ) as vld:
        validator = json.load(vld)

    if not config["nodes"][0].get("management", {}).get("ipaddress"):
        try:
            jsonschema.validate(config, validator)
        except Exception as error:
            message = getattr(error, "message", str(type(error)))
            tree = ""
            for seq in getattr(error, "path", list()):
                if isinstance(seq, int):
                    tree += "/<sequence {}>".format(seq)
                else:
                    tree += "/{}".format(seq)

            validator = getattr(error, "validator")
            if validator == "type":
                message = "{msg} at {tree}".format(msg=message, tree=tree)
            elif validator == "required":
                message = "{msg} in config at {tree}. Please check example config for reference.".format(
                    msg=message, tree=tree
                )
            raise RuntimeError(message)

    is_conv = is_conv_env(config)
    if not is_conv:
        for node in config["nodes"]:
            if "ipmi" not in node:
                raise RuntimeError(
                    "IPMI section required for node {}".format(node["name"])
                )
            elif "management" not in node and "controller" not in node["roles"]:
                raise RuntimeError(
                    "Management section required for node {}".format(node["name"])
                )

    for key, value in config["network"].items():
        if "network" in value:
            _helper(config["nodes"])
    key = config["ssh"]["private-key"]
    public_key = local_execute(
        ["ssh-keygen", "-y", "-f", "/dev/stdin"], input=key.encode("utf-8")
    )
    config["ssh"]["public-key"] = public_key
    return config


def get_versions(config, version=None, url=None):
    """
    Loads the version manifests either from the existing cluster or downloads it from the provided url
    param version ,, str version number to get from default 0-complexity/home/manifests location
    param url ,, raw url to version-manifests yaml file
    """

    def download_load_yaml(url):
        response = requests.get(url)
        response.raise_for_status()
        versions = yaml.load(response.content)
        versions["url"] = url
        versions["version"] = version
        return versions

    if not version:
        if not url:
            first_node = get_pnodes(config, roles=["controller"], active_only=True)[
                0
            ].remote
            res = first_node.run(
                "kubectl get configmap versions-manifest -o json", check=False
            )
            if res.exit_status != 0:
                if "NotFound" in res.stderr:
                    raise RuntimeError(
                        "configmap manifest does not exist , version needs to be passed during deploy cluster to be registered."
                    )
                raise RuntimeError(res.stderr)
            versions = yaml.load(
                json.loads(res.stdout)["data"]["versions-manifest.yaml"]
            )
        else:
            versions = download_load_yaml(url)
    else:
        url = "https://raw.githubusercontent.com/0-complexity/home/master/manifests/{0}.yml".format(
            version
        )
        versions = download_load_yaml(url)

    if 404 in versions:
        raise RuntimeError("Could not find manifest @ {}".format(url))
    for repo in versions["repos"]:
        if repo["url"] == "https://github.com/0-complexity/openvcloud":
            versions["ovcversion"] = version_type(repo)
            break
    return versions


def get_controller_ips(config, iptype="management"):
    for node in config["nodes"]:
        if "controller" in node["roles"]:
            yield get_ipaddress(node[iptype]["ipaddress"])


class AbstractConfig:
    def __init__(self, config, versions=None):
        self.config = config
        self._nodes = None
        self.versions = versions
        self.backplaneips = list(get_controller_ips(config, "backplane"))
        self.managementips = list(get_controller_ips(config, "management"))
        self.is_conv_env = is_conv_env(config)

    @property
    def nodes(self):
        if self._nodes is None:
            nodes = []
            for address in get_controller_ips(self.config, "backplane"):
                client = remote.Remote(
                    address, username="root", pkey=self.config["ssh"]["private-key"]
                )
                nodes.append(client)
            self._nodes = nodes
        return self._nodes

    @property
    def first_node(self):
        return self.nodes[0]

    def kubectl(self, *args, **kwargs):
        cmd = "kubectl "
        cmd += " ".join(args)
        return self.first_node.run(cmd, **kwargs)

    def get_node_from_appname(self, appname):
        output = self.kubectl("get pods -o json -l app=={}".format(appname))
        data = json.loads(output.stdout)
        for pod in data["items"]:
            hostip = pod["status"]["hostIP"]
            for node in self.nodes:
                net = node.run("ip a").stdout
                netinfo = network.parse_net_info(net)
                for nic in netinfo:
                    for ip in nic["ip"]:
                        if ip == hostip:
                            return node

            raise LookupError("Could not find host with IP {}".format(hostip))
        raise LookupError("Could not find appname {}".format(appname))

    def get_base_image(self):
        log("Getting base image")
        for image, tag in self.versions["images"].items():
            if "openvcloud/base" in image:
                break
        else:
            raise RuntimeError("Could not find base image in versions manifest")
        self.first_node.run("mkdir -p {}".format(BASE_FS_PATH))
        fs_path = BASE_FS_PATH + "image_fs_{}_{}".format(image.replace("/", "_"), tag)
        exclude_cmd = ""
        for path in BASE_IMAGE_EXCLUDE:
            exclude_cmd += " --exclude {}".format(path)
        cmd = """
        mkdir -p {1}
        docker pull {0}:{3}
        docker rm -f install_base || true # delete image if still exists from older version
        docker create --name install_base {0} true
        docker export install_base | tar xf - -C {1}{2}
        """.format(
            image, fs_path, exclude_cmd, tag
        )
        self.first_node.run(cmd)
        return fs_path

    def clean_base_image_path(self, fs_path):
        cmd = """
        docker rm -f install_base || true
        docker image prune -f
        rm -rf {}
        """
        self.first_node.run(cmd.format(fs_path))

    def remove_env_keys(self, node, public_key):
        """
        Remove unwanted public keys from authorized keys under /root/.ssh/authorized_keys.
        """
        auth_keys = node.read_file("/root/.ssh/authorized_keys")
        data = []
        for key in auth_keys.splitlines():
            if key.startswith(public_key.strip()):
                continue
            data.append(key)
        data.append("")  # end with empty line
        node.write_file("/root/.ssh/authorized_keys", "\n".join(data))

    def add_env_keys(self, node, pub_key, priv_key):
        """
        adds pub and priv key to relative envs
        """
        # create dir
        ssh_dirs = ["/var/ovc", "/root"]
        node.authorize_key("root", pub_key)
        for ssh_dir in ssh_dirs:
            node.run("mkdir -p {0}/.ssh && chmod 600 {0}/.ssh".format(ssh_dir))
            node.run(
                "mkdir -p {0}/root/.ssh && chmod 600 {0}/root/.ssh".format(ssh_dir)
            )
            # add keys
            node.write_file("{}/.ssh/id_rsa".format(ssh_dir), priv_key)
            node.write_file("{}/.ssh/id_rsa.pub".format(ssh_dir), pub_key)
            chmod_command = """
            chmod 600 {0}/.ssh/id_rsa.pub
            chmod 600 {0}/.ssh/id_rsa
            """.format(
                ssh_dir
            )
            node.run(chmod_command)
            if not node.exists("{}/.ssh/authorized_keys".format(ssh_dir)):
                node.write_file("{}/.ssh/authorized_keys".format(ssh_dir), pub_key)
        node.run("mkdir -p {0} && chmod 777 {0}".format("/var/ovc/pxeboot/images/"))
        node.write_file("/var/ovc/pxeboot/images/pubkey", pub_key)
        node.run("chmod 777 /var/ovc/pxeboot/images/pubkey")

    def get_new_keys(self):
        privatekey = self.first_node.read_file("/var/ovc/.ssh/id_rsa")
        pubkey = self.first_node.read_file("/var/ovc/.ssh/id_rsa.pub")
        return privatekey, pubkey

    def configure_keys(self):
        log("Configuring new keys")
        if not len(self.nodes):
            raise RuntimeError("cluster needs to be depployed before configuring keys")

        # generate keys
        private_key_path = "/root/.ssh/id_rsa"
        public_key_path = "{}.pub".format(private_key_path)
        if not self.first_node.exists(private_key_path):
            self.first_node.run(
                "ssh-keygen -q -t rsa -f {} -N ''".format(private_key_path)
            )
        key_perms = """
        chmod 600 {0}
        chown root:root {0}
        """
        self.first_node.run(key_perms.format(public_key_path))
        self.first_node.run(key_perms.format(private_key_path))

        # add keys to kuberntes
        public_key = self.first_node.read_file(public_key_path)
        private_key = self.first_node.read_file(private_key_path)

        # add keys to controller nodes
        for node in self.nodes:
            self.add_env_keys(node, public_key, private_key)


class Node(AbstractConfig):
    def __init__(self, config, name, versions, fs_path=None):
        super().__init__(config, versions)
        self.name = name
        self._node = None
        self.managementip = get_ipaddress(self.node["management"]["ipaddress"])
        self._remote = None
        self.config = config
        self._roles = None
        self.password = config["environment"]["password"]
        self.gid = config["environment"]["grid"]["id"]
        self.fqdn = "{}.{}".format(
            config["environment"]["subdomain"], config["environment"]["basedomain"]
        )
        self.iyourl = "https://itsyou.online/"
        self.fs_path = fs_path

    def log(self, msg):
        if ISINTERACTIVE and self.name in status:
            status[self.name] = msg
            print_status(True)
        else:
            msg = "{}: {}".format(self.name, msg)
            log(msg)

    @property
    def remote(self):
        if self._remote is None:
            self._remote = remote.Remote(
                self.managementip,
                username="root",
                pkey=self.config["ssh"]["private-key"],
            )
        return self._remote

    @property
    def node(self):
        if self._node is None:
            for node in self.config["nodes"]:
                if node["name"] == self.name:
                    self._node = node
                    break
            else:
                raise LookupError("Faild to find a node with name {}".format(self.name))
        return self._node

    @property
    def roles(self):
        if self._roles is None:
            for node in self.config["nodes"]:
                if node["name"] == self.name:
                    self._roles = node["roles"]
                    break
            else:
                raise LookupError(
                    "Failed to find a node with name {}".format(self.name)
                )
        return self._roles


class JumpScale7(Node):
    def copy_base_image(self):
        self.log("Getting node info")
        res = portal_api_call(
            "system/gridmanager/getNodes",
            self.config,
            data={"ipaddr": self.remote.address},
        )
        node_data = next(iter(res.json()), {})
        grid_yaml = {
            "id": self.config["environment"]["grid"]["id"],
            "node": {
                "id": node_data.get("id"),
                "machineguid": node_data.get("machineguid"),
                "roles": node_data.get("roles", ['node']),
            },
        }
        self.log("Syncing base image")
        self.first_node.add_ssh_host("root", self.remote.address)
        self.first_node.run(
            "rsync -av --delete {}/opt/jumpscale7/ {}:/opt/jumpscale7/".format(
                self.fs_path, self.remote.address
            )
        )
        self.first_node.run(
            "rsync -av --exclude opt/jumpscale7 {}/ {}:/".format(
                self.fs_path, self.remote.address
            )
        )
        grid_yaml = yaml.dump(grid_yaml, default_flow_style=False)
        self.remote.run("mkdir -p /opt/jumpscale7/cfg/system")
        self.remote.write_file("/opt/jumpscale7/cfg/system/grid.yml", grid_yaml)

    def get_services(self):
        services = ["openvcloud_redis", "openvcloud_jsagent"]
        if "cpu" in self.roles or "storage" in self.roles:
            services.append("nginx")
        if "cpu" in self.roles:
            services.append("openvcloud_libvirtlistener")
            services.append("openvcloud_vncproxy")
        return services

    def start(self):
        self.log("Start JumpScale services")
        self.remote.run("systemctl start {}".format(" ".join(self.get_services())))

    def stop(self, ays=False):
        self.log("Stop JumpScale services")
        if ays:
            self.remote.run("ays stop", check=False)
            self.remote.run("systemctl disable jsagent_main.service", check=False)
            self.remote.run("rm -f /etc/init.d/ays")
            self.remote.run("rm -f /usr/local/bin/ays")
            self.remote.run("rm -f /etc/systemd/system/jsagent_main.service")
        else:
            for service in self.get_services():
                res = self.remote.run(
                    "systemctl status {}".format(service), check=False
                )
                if res.exit_status == 0:
                    self.remote.run("systemctl stop {}".format(service))

    def upgrade(self):
        extra_flag = "--update"
        ays = False
        res = self.remote.run("which ays", check=False)
        if res.exit_status == 0:
            ays = True
            extra_flag = ""
        try:
            self.stop(ays)
            self.copy_base_image()
            self._run_install_node(extra_flag)
        finally:
            self.start()

    def restart(self):
        self.stop()
        self.start()

    def _run_install_node(self, extra_flags=""):
        extra_args = {
            "vxvlan": self.config["network"]["vxbackend"]["vlan"],
            "vxip": self.node["vxbackend"]["ipaddress"],
            "gwvlan": self.config["network"]["gateway-management"]["vlan"],
            "gwip": self.node["gateway-management"]["ipaddress"],
            "gridid": self.config["environment"]["grid"]["id"],
            "envsub": self.config["environment"]["subdomain"],
            "envcli": self.config["itsyouonline"]["clientId"],
            "envsec": self.config["itsyouonline"]["clientSecret"],
        }
        self.remote.run(
            """/opt/jumpscale7/bin/installnode {} --roles={} --masterips {} --password={} --gid={} --fqdn={} \
            --vxbackend_vlan={vxvlan} --vxbackend_ip={vxip} --gwmgmt_vlan={gwvlan} --gwmgmt_ip={gwip} \
            --grid_id={gridid} --env_subdomain={envsub} --client_id={envcli} --secret={envsec}""".format(
                extra_flags,
                " ".join(self.roles),
                " ".join(self.backplaneips),
                self.password,
                self.gid,
                self.fqdn,
                **extra_args
            )
        )

    def install(self):
        self.copy_base_image()
        self._run_install_node()


class Cluster(AbstractConfig):
    """
    Cluster abstraction layer to allow for easier manipulation.
    """

    def __init__(self, config_path, versions, development=False):
        super().__init__(config_path, versions)
        self.k8s_config = "/root/.kube/{}.conf".format(
            self.config["environment"]["subdomain"]
        )
        self.join_line = ""
        self.scripts_dir = "{}/kuberesources/".format(REPO_PATH)
        self.development = development

    def write_certificates(self):
        log("Writing certificates")
        ssldir = "/var/ovc/ssl/"

        cert_items = ["crt", "key"]
        for cert_name in self.config["environment"]["ssl"].values():
            # get certificates from config
            for item in cert_items:
                value = self.config["certificates"][cert_name][item]
                for node in self.nodes:
                    file = "{}{}.{}".format(ssldir, cert_name, item)
                    node.write_file(file, value)

    def reset(self):
        if self.first_node.exists("/etc/kubernetes/admin.conf"):
            self.kubectl("delete deployment pxeboot", check=False)
        for node in self.nodes:
            node.run("kubeadm reset")
            node.run("service etcd stop")

    def install_kubernetes_cluster(self, kube_client):
        """
        Will install a kubernetes master and minion nodes on the first and rest of the node list respectively.
        """
        log("Installing k8 cluster")
        for node in self.nodes:
            node.run("sed -i.bak '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab")
        etcdcluster.install_cluster(self.nodes)
        self.join_line, k8s_config_data = kube_client.install_kube_masters(
            self.nodes,
            unsafe=True,
            external_ips=self.managementips,
            flannel=False,
            kube_cidr="10.244.0.0/16",
        )

        # configure cgroups
        if self.config.get("limits"):
            if self.config["limits"].get("pods"):
                cpu_set = self.config["limits"]["pods"]["cpu"]
                memory = self.config["limits"]["pods"]["memory"]
                for node in self.nodes:
                    set_cgroup(node, "kubepods", cpu_set, memory)

        if self.is_conv_env:
            pub_key = self.first_node.read_file("/root/.ssh/default.pub").strip()
            # authorize non controller nodes
            cpu_nodes = get_pnodes(self.config, roles=["cpu"])
            for node in cpu_nodes:
                node.remote.authorize_key("root", pub_key)
                user = self.first_node.run("whoami").stdout
                self.first_node.add_ssh_host(user, node.remote.address)
        os.makedirs("/root/.kube", exist_ok=True)
        with open(self.k8s_config, "w") as cfg:
            cfg.write(k8s_config_data)
        if not os.path.exists("/root/.kube/config"):
            with open("/root/.kube/config", "w") as cfg:
                cfg.write(k8s_config_data)

    def clone_repo(self, url=REPO_URL):
        parse = urllib.parse.urlparse(url)
        hostname = parse.hostname[: parse.hostname.rfind(".")]
        dest = "/opt/code/{host}{path}".format(host=hostname, path=parse.path)
        for node in self.nodes:
            if not node.exists(dest):
                node.run("mkdir -p {}".format(dest))
                node.run("git clone {url} {dest}".format(url=url, dest=dest))

    def _install_kubectl(self, kube_client, node=None):
        loc = "/usr/local/bin/kubectl"
        if node:
            exists = node.exists(loc)
        else:
            exists = os.path.exists(loc)
        if not exists:
            kube_client.install_kube_client(node, "/usr/local/bin/kubectl")

    def verify_certificates(self):
        for key, cert_name in self.config["environment"]["ssl"].items():
            cert = self.config["certificates"][cert_name]
            if not cert:
                raise RuntimeError("Certificate {} not found.".format(cert_name))
            for extension in ("crt", "key"):
                if not cert.get(extension):
                    raise RuntimeError(
                        'ERROR: "{}" is required property in config /certificates/{}/'.format(
                            extension, cert_name
                        )
                    )

    def install_controller(self, kube_client):
        """
        Will use existing yaml or config scripts in this dir as well as jumpscale modules to install the controller setup on the
        cluster. Creating the relevant deployments, services, and mounts
        """
        log("Install kubectl")
        for node in self.nodes:
            self._install_kubectl(kube_client, node)
        self._install_kubectl(kube_client)

        self.prepare_directories()
        self.write_certificates()
        self.configure_keys()
        output = self.kubectl("get secret -o json")
        secrets = json.loads(output.stdout)
        for secret in secrets["items"]:
            if secret["metadata"]["name"] == "ovs-cred":
                break
        else:
            self.generate_edge_password()
        self.kube_deploy()
        for node in self.nodes:
            self.remove_env_keys(node, self.config["ssh"]["public-key"])

    def prepare_directories(self):
        log("Preparing directories")
        dirmodmap = {}
        for dirmap in self.config["directories"]:
            chmod = dirmap.get("chmod", 0o777) or 0o777
            dirmodmap.setdefault(chmod, []).append(dirmap["path"])

        for node in self.nodes:
            for mod, paths in dirmodmap.items():
                node.run("mkdir -p {0} && chmod {1:o} {0}".format(" ".join(paths), mod))

    def apply_template(self, template, kind):
        self.kubectl("apply -f {template}".format(template=template))
        template_name = os.path.basename(template)
        timeout = time.time() + 240
        print("Waiting for {} to be ready..".format(template_name))
        while time.time() < timeout:
            template_basename = template.split("/")[-1]
            template_name = (
                os.path.splitext(template_basename)[0]
                if template_basename.endswith(".yaml")
                else template_basename
            )
            out = self.kubectl("get %s %s -o json" % (kind, template_name))
            json_out = json.loads(out.stdout)
            ready = False
            if kind == "deployment":
                for condition in json_out["status"].get("conditions", []):
                    if (
                        condition["type"] == "Available"
                        and condition["status"] == "True"
                    ):
                        ready = True
                        break
            if kind == "statefulset":
                if json_out["status"]["replicas"] == json_out["status"].get(
                    "readyReplicas"
                ):
                    ready = True
                    break
            if ready:
                break
            time.sleep(2)
        else:
            raise TimeoutError(
                "Deploying {} took longer than expected. Exiting..".format(
                    template_name
                )
            )

    def dump(self, location, data, dumper="yaml"):
        if dumper == "yaml":
            data = yaml.dump(
                data,
                default_flow_style=False,
                default_style="",
                indent=4,
                line_break="\n",
            )
        elif dumper == "json":
            data = json.dumps(data, ensure_ascii=False, sort_keys=False, indent=False)
        else:
            raise RuntimeError("dumper value either json or yaml")
        self.first_node.write_file(location, data)

    def load(self, location, loader=yaml.load):
        data = self.first_node.read_file(location)
        return loader(data)

    def write_service_template(self, template, externalips):
        template_loc = "{dir}{name}/{name}-service.yaml".format(
            dir=self.scripts_dir, name=template
        )
        data = self.load(template_loc)
        if data["spec"].get("type") == "NodePort":
            data["spec"]["externalIPs"] = externalips
            self.dump(template_loc, data)
        return template_loc

    def _reapply_service(self, name):
        pubips = list(get_controller_ips(self.config, "fallback"))
        loc = self.write_service_template(name, pubips)
        self.kubectl("apply -f {}".format(loc))

    def upgrade(self):
        self.prepare_directories()
        self.prepare_templates()
        self._reapply_service("upgrader")

    def revert(self):
        """
        Reconfigure nginx service to working state
        """
        self._reapply_service("nginx")

    def prepare_templates(self):
        log("Preparing resources")
        srcdir = "{}/scripts/kubernetes/".format(REPO_PATH)

        def patch_for_development(data):
            for container in data["spec"]["template"]["spec"]["containers"]:
                volmounts = container.setdefault("volumeMounts", [])
                volmounts.append(
                    {
                        "name": "code",
                        "mountPath": "/opt/code/github/0-complexity",
                        "subPath": "github/0-complexity",
                    }
                )
                volmounts.append(
                    {
                        "name": "code",
                        "mountPath": "/opt/code/github/jumpscale7",
                        "subPath": "github/jumpscale7",
                    }
                )
            volumes = data["spec"]["template"]["spec"].setdefault("volumes", [])
            volumes.append(
                {"name": "code", "hostPath": {"path": "/opt/code", "type": "Directory"}}
            )
            data["spec"]["template"]["spec"]["nodeSelector"] = {
                "kubernetes.io/hostname": self.first_node.hostname
            }

        def update_template(template):
            types = ("Deployment", "StatefulSet", "Job")
            with open(template, "r") as tmp:
                data = yaml.load(tmp)
            changed = False
            if isinstance(data, dict) and data.get("kind") in types:
                for conttype in ("initContainers", "containers"):
                    if conttype not in data["spec"]["template"]["spec"]:
                        continue
                    for container in data["spec"]["template"]["spec"][conttype]:
                        # get image version from the self.versions dict loaded from 0-complexity/home
                        if container["image"] in self.versions["images"]:
                            container["image"] = "{}:{}".format(
                                container["image"],
                                self.versions["images"][container["image"]],
                            )
                            changed = True
                for volume in data["spec"]["template"]["spec"].get("volumes", []):
                    repo = volume.get("gitRepo")
                    if repo:
                        update_repo_volume(self.versions, repo)
                        changed = True
                if (
                    data["spec"]["template"]
                    .get("metadata", {})
                    .get("labels", {})
                    .get("app")
                    in DEV_TMPLS
                ):
                    if self.development:
                        changed = True
                        patch_for_development(data)

                if changed:
                    self.dump(template.replace(srcdir, self.scripts_dir), data)

        self.first_node.copy_dir_tree(srcdir, self.scripts_dir)
        # add external ips to the specified services to expose to the internal network
        templates = ["agentcontroller", "osis", "management"]
        externalips = list(get_controller_ips(self.config, "backplane"))
        for template in templates:
            self.write_service_template(template, externalips)
        # add external ips to the specified services to expose to the internet
        pubtemplates = ["nginx", "zero-access"]
        pubips = list(get_controller_ips(self.config, "fallback"))
        for template in pubtemplates:
            self.write_service_template(template, pubips)
        # add code to mount specified repos for in developmnent mode
        if self.development:
            for repo in DEV_REPOS:
                self.clone_repo(repo)
        for path, _, filenames in os.walk(srcdir):
            for file in filenames:
                if file.endswith(".yaml"):
                    update_template("{}/{}".format(path, file))

        stat_template = self.scripts_dir + "/stats-collector/stats-deployment.yaml"
        data = self.load(stat_template)
        data["spec"]["template"]["spec"]["containers"][0]["args"][3] = self.config[
            "network"
        ]["management"]["network"]
        self.dump(stat_template, data)

    def write_config_map(self, name, content):
        if isinstance(content, (dict, list)):
            content = yaml.dump(content, default_style="|")
        self.first_node.run(
            "cat << EOF | kubectl create configmap {0} --dry-run --from-file={0}.yaml=/dev/stdin -o json | kubectl apply -f -\n{1}\nEOF".format(
                name, content
            )
        )

    def write_system_config(self):
        log("Write system config")
        self.write_config_map("system-config", self.config)
        versions = copy.deepcopy(self.versions)
        versions.pop("ovcversion", None)
        self.write_config_map("versions-manifest", versions)

    def install_teleport(self):
        for node in self.nodes:
            log("Installing teleport {}".format(node.hostname))
            ssl_name = self.config["environment"]["ssl"]["root"]
            teleport_client = teleport.Teleport(node)
            if (
                not node.exists("/usr/local/bin/teleport")
                or not node.exists("/usr/local/bin/tctl")
                or not node.exists("/usr/local/bin/tsh")
            ):
                teleport_client.install()
            teleport_client.write_config(
                name=node.hostname,
                key_path="/var/ovc/ssl/%s.key" % ssl_name,
                cert_path="/var/ovc/ssl/%s.crt" % ssl_name,
            )
            teleport_client.restart()
            time.sleep(3)
            github = self.config["support"]["github"]
            teams = [
                "/".join([team["org_name"], team["team_name"]])
                for team in github["teams"]
            ]
            resources = node.run("tctl get github/%s" % node.hostname).stdout
            if node.hostname in resources:
                continue
            teleport_client.apply_permissions(
                name=node.hostname,
                client_id=github["client_id"],
                client_secret=github["client_secret"],
                teams=teams,
                exposed_ip="{}.{}".format(
                    self.config["environment"]["subdomain"],
                    self.config["environment"]["basedomain"],
                ),
            )

    def kube_deploy(self):
        self.prepare_templates()
        self.write_system_config()

        self.kubectl("apply -f {path}/rbac.yaml".format(path=self.scripts_dir))
        self.kubectl(
            "apply -f {path}/flannel/flannel.yml".format(path=self.scripts_dir)
        )
        deployments = [
            "influxdb",
            "osis",
            "agentcontroller",
            "stats-collector",
            "portal",
            "pxeboot",
            "zero-access",
            "management",
            "ovs",
            "controller-jsagent",
            "nginx",
        ]
        statefulSets = ["mongo", "syncthing"]
        if self.development:
            deployments = DEV_TMPLS
            statefulSets = []
        elif self.is_conv_env:
            deployments.remove("pxeboot")
        for statefulSet in statefulSets:
            template_file = self.scripts_dir + statefulSet
            self.apply_template(template_file, kind="statefulset")
        for deployment in deployments:
            template_file = self.scripts_dir + deployment
            self.apply_template(template_file, kind="deployment")
        if not self.development:
            self.grafana_apply("{}/grafana".format(self.scripts_dir))

    def grafana_apply(self, grafana_dir):
        log("Installing grafana")
        datasourcepath = "{}/provisioning/datasources/influx.yaml".format(grafana_dir)
        dashboardpath = "{}/sources/templates".format(grafana_dir)
        datasource = self.load(datasourcepath)
        datasourcename = "controller_{}".format(self.config["environment"]["subdomain"])
        datasource["datasources"][0]["name"] = datasourcename
        self.dump(datasourcepath, datasource)
        for loc, _, dashboardfiles in self.first_node.walk(
            dashboardpath, pattern="*.json"
        ):
            for dashboardfile in dashboardfiles:
                dashboardfile = os.path.join(loc, dashboardfile)
                db = self.load(dashboardfile, json.loads)
                db["id"] = None
                db["title"] += " ({})".format(self.config["environment"]["subdomain"])
                for row in db["rows"]:
                    for panel in row["panels"]:
                        panel["datasource"] = datasourcename
                if "templating" in db:
                    for item in db["templating"]["list"]:
                        item["datasource"] = datasourcename
                self.dump(dashboardfile, db, "json")
        cmd = """
        set -e
        cd {dir}
        kubectl create configmap grafana-provisioning-datasources --from-file=provisioning/datasources
        kubectl create configmap grafana-provisioning-dashboards --from-file=provisioning/dashboards
        kubectl create configmap grafana-dashboards --from-file=sources/templates
        kubectl apply -f grafana-service.yaml
        kubectl apply -f grafana-deployment.yaml
        """.format(
            config=self.k8s_config, dir=grafana_dir
        )
        self.first_node.run(cmd)

    def delete(self):
        log("Deleting configmaps")
        self.kubectl("delete configmap --all --namespace default")
        self.write_system_config()
        log("Deleting services")
        self.kubectl("delete service --namespace default -l 'name!=nginx,name!=qa'")
        log("Deleting statefulset")
        self.kubectl("delete statefulset --all --namespace default")
        log("Deleting deployments")
        self.kubectl("delete deploy --namespace default -l 'app!=qa'")
        self.kubectl("delete deploy --all --namespace development")

    def init_ovscred(self):
        content = """
from JumpScale import j
scl = j.clients.osis.getNamespace("system")
grid = scl.grid.get(j.application.whoAmI.gid)
ovs_credentials = grid.settings.get("ovs_credentials", {})
edgeuser = ovs_credentials.get("edgeuser")
edgepassword = ovs_credentials.get("edgepassword")
print("{}, {}".format(edgeuser, edgepassword))
        """
        self.first_node.write_file("/tmp/getovscred.py", content)
        edgeuser, edgepassword = self.first_node.run(
            "jspython /tmp/getovscred.py"
        ).stdout.split(",", 1)
        self.kubectl(
            "create secret generic ovs-cred --from-literal=edgeuser={} --from-literal=edgepassword={} --dry-run -o yaml | kubectl apply -f  -".format(
                edgeuser.strip(), edgepassword.strip()
            )
        )

    def update(self):
        output = self.kubectl("get secret -o json")
        secrets = json.loads(output.stdout)
        for secret in secrets["items"]:
            if secret["metadata"]["name"] == "ovs-cred":
                break
        else:
            self.init_ovscred()

        log("Restarting cluster...")
        if self.development:
            self.kubectl("delete deploy {}".format(" ".join(DEV_TMPLS)), check=False)
        else:
            self.delete()
        self.kube_deploy()

    def generate_edge_password(self):
        chars = string.digits + string.ascii_letters
        username = [random.choice(chars) for x in range(9)]
        chars += string.punctuation
        password = [random.choice(chars) for x in range(11)]
        self.kubectl(
            "create secret generic ovs-cred --from-literal=edgeuser={} --from-literal=edgepassword={} --dry-run -o yaml | kubectl apply -f  -".format(
                username, password
            )
        )


class PNode(JumpScale7):
    def ipmi(self, *args):
        ipmiip = get_ipaddress(self.node["ipmi"]["ipaddress"])
        if "controller" in self.node["roles"]:
            network = netaddr.IPNetwork(self.config["network"]["ipmi"]["network"])
            ipmiip = str(network.broadcast - self.node["ip-lsb"])

        cmd = "ipmitool -I lanplus -H {} -U {} -P {} ".format(
            ipmiip, self.node["ipmi"]["username"], self.node["ipmi"]["password"]
        )
        cmd += " ".join(args)
        first_node = get_pnodes(self.config, roles=["controller"], active_only=True)[
            0
        ].remote
        first_node.run(cmd)

    def power_on(self):
        self.log("Starting")
        self.ipmi("chassis power on")

    def shutdown(self):
        self.log("Shutting down")
        self.ipmi("chassis power soft")

    def force_shutdown(self):
        self.log("Shutting down")
        self.ipmi("chassis power off")

    def reboot(self):
        self.log("Rebooting")
        self.ipmi("chassis power cycle")

    def is_up(self):
        return network.tcp_test(self.managementip, 22)

    def wait_up(self, timeout=600):
        network.wait_for_connection(self.managementip, 22, timeout)
        if not self.is_up():
            raise RuntimeError("Node did not come up in {}seconds".format(timeout))

    def enable_pxe(self):
        self.log("Enable PXE")
        node = self.get_node_from_appname("pxeboot")
        self.ipmi("chassis bootdev pxe")
        macaddress = str(netaddr.EUI(self.node["management"]["macaddress"])).lower()
        dst = "/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}".format(macaddress)
        src = "../../conf/tftp-911boot"
        if not node.exists(dst):
            node.sftp.symlink(src, dst)

    def disable_pxe(self):
        self.log("Disable PXE")
        node = self.get_node_from_appname("pxeboot")
        macaddress = str(netaddr.EUI(self.node["management"]["macaddress"])).lower()
        dst = "/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}".format(macaddress)
        if node.exists(dst):
            node.sftp.unlink(dst)

    def install_os(self):
        for retry in range(0, 3):
            try:
                self._install_os()
            except:
                if retry != 2:
                    time.sleep(10)
                    continue
                raise
            break

    def _install_os(self):
        self.enable_pxe()
        self.reboot()
        time.sleep(20)
        self.log("Waiting for 911 boot")
        self.wait_up()
        self.log("Installing OS")
        status = self.remote.run("cd /root/tools/; ./Install").exit_status
        if status != 0:
            raise RuntimeError("Failed to install OS")
        self.disable_pxe()
        self.reboot()
        time.sleep(20)
        self.log("Waiting for OS boot")
        self.wait_up()


###                    ###
# Command line interface #
###                    ###
@click.group()
@click.option("--config", help="Config file to deploy the cluster", envvar="ENV_CONFIG")
@click.option(
    "--version",
    help="version number to install or update to",
    envvar="OVC_VERSION",
    default=None,
)
@click.option(
    "--url", help="version url to install from", envvar="OVC_VERSION_URL", default=None
)
@click.option("--loglevel", default=50, help="Set loglevel", type=int)
@click.pass_context
def cli(ctx, config, version, url, loglevel):
    ctx.obj = {}
    if not config:
        raise LookupError("Please specify a config file")
    config = prepare_config(config)
    ctx.obj["config"] = config
    ctx.obj["versions"] = get_versions(config, version, url)


@cli.group()
def cluster():
    pass


@cluster.group(help="Manipulation of kubernetes resources")
def resources():
    pass


@cli.group()
def node(help="Node actions"):
    pass


@cli.group()
def storage(help="Storage actions"):
    pass


@node.command("action", help="Will apply the action on storage and cpu nodes")
@click.option(
    "--name", help="Node name pass all to apply on all comma seperate list for multiple"
)
@click.argument(
    "action",
    type=click.Choice(
        [
            "reboot",
            "power_on",
            "shutdown",
            "force_shutdown",
            "is_up",
            "wait_up",
            "enable_pxe",
            "disable_pxe",
            "install_os",
        ]
    ),
)
@click.pass_context
def node_action(ctx, name, action):
    """
    Execute command for node
    """
    if name != "all" and "," not in name:
        cluster = PNode(ctx.obj["config"], name, ctx.obj["versions"])
        getattr(cluster, action)()
    else:
        if name == "all":
            pnodes = get_pnodes(
                ctx.obj["config"], ctx.obj["versions"], roles=["cpu", "storage"]
            )
        else:
            pnodes = [
                PNode(ctx.obj["config"], pname, ctx.obj["versions"])
                for pname in name.split(",")
            ]
        pnode_action(pnodes, action)


@node.command(
    "jsaction", help="Will apply the action on all nodes (cpu, storage, controller)"
)
@click.option(
    "--name", help="Node name pass all to apply on all comma seperate list for multiple"
)
@click.argument(
    "action", type=click.Choice(["start", "stop", "restart", "install", "upgrade"])
)
@click.pass_context
def node_jsaction(ctx, name, action):
    """
    Execute command for node
    """
    if action in ["upgrade", "install"]:
        cluster = AbstractConfig(ctx.obj["config"], ctx.obj["versions"])
        fs_path = cluster.get_base_image()
    if name == "all":
        pnodes = get_pnodes(
            ctx.obj["config"],
            ctx.obj["versions"],
            roles=["cpu", "storage", "controller"],
            fs_path=fs_path,
        )
    else:
        pnodes = [
            PNode(ctx.obj["config"], pname, ctx.obj["versions"], fs_path)
            for pname in name.split(",")
        ]
    try:
        pnode_action(pnodes, action)
    finally:
        if action in ["upgrade", "install"]:
            cluster.clean_base_image_path(fs_path)


@node.command("update", help="Update code and restart required services on node")
@click.pass_context
def node_update(ctx):
    return _node_update(ctx)


def _node_update(ctx):
    """
    Update all pnodes
    """
    config = ctx.obj["config"]
    pnodes = get_pnodes(
        config, ctx.obj["versions"], roles=["cpu", "storage", "controller"]
    )
    pnode_action(pnodes, "upgrade")
    pnode_action(pnodes, "restart")


@resources.command("writeconfig", help="Write system-config to kubernetes")
@click.pass_context
def write_system_config(ctx):
    """
    Write or update system-config in kubernetes configmap based on yaml file
    """
    cluster = Cluster(ctx.obj["config"], ctx.obj["versions"])
    cluster.write_system_config()


@resources.command("write", help="Rewrite all templates")
@click.option("--development", default=False, is_flag=True)
@click.pass_context
def write_kube_resources(ctx, development):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj["config"], ctx.obj["versions"], development)
    cluster.prepare_templates()


@resources.command("applyall", help="Apply all kubernetes resources")
@click.pass_context
def kube_apply_all(ctx):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj["config"], ctx.obj["versions"])
    cluster.kube_deploy()


@resources.command("apply", help="Apply a specifified kubernetes resource")
@click.option("--path", help="path to template")
@click.pass_context
def kube_apply(ctx, path):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj["config"], ctx.obj["versions"])
    cluster.apply_template(path, kind="deployment")


@cluster.command("deploy", help="Deploy kubernetes cluster on controller nodes")
@click.option(
    "--configure-cluster/--no-configure-cluster",
    help="Configure kubernetes cluster",
    default=True,
)
@click.option(
    "--force",
    help="Reset installed environment for redployment",
    default=False,
    is_flag=True,
)
@click.pass_context
def deploy_cluster(ctx, configure_cluster, force):
    """
    Deploy will create the cluster machines and deploy kubernetes cluster on top of them.
    """
    cluster = Cluster(ctx.obj["config"], ctx.obj["versions"])
    cluster.verify_certificates()
    kube_client = kubernetes.Kubernetes()
    if force:
        cluster.reset()
    if configure_cluster:
        cluster.install_kubernetes_cluster(kube_client)
    cluster.install_controller(kube_client)


@cluster.command("update", help="Update cluster, delete existing resources and reapply")
@click.option("--development", default=False, is_flag=True)
@click.pass_context
def update_cluster(ctx, development):
    return _update_cluster(ctx, development)


def _update_cluster(ctx, development):
    """
    Will update the cluster.
    """
    cluster = Cluster(ctx.obj["config"], ctx.obj["versions"], development)
    cluster.update()


def _preFlightCheck(ctx):
    """
    Check if the environment is ready for upgrade
    """
    config = ctx.obj["config"]
    nodes_status = get_nodes_status(config)

    pnodes = get_pnodes(
        config, versions=ctx.obj["versions"], roles=["cpu", "storage", "controller"]
    )

    offline_nodes = list()
    for node in pnodes:
        if node.name in nodes_status.keys():
            if nodes_status[node.name] == "ENABLED" and not node.is_up():
                offline_nodes.append(node.name)

            if "controller" in node.roles and nodes_status[node.name] != "ENABLED":
                print(
                    "Cannot perform upgrade, controller node {} is not enabled".format(
                        node.name
                    )
                )
                sys.exit(1)

        else:
            print("Couldn't find node {}".format(node.name))
            sys.exit(1)

    if offline_nodes:
        print(
            "Can't connect to the following node(s): {}. Please either investigate the issue or put the node(s) in maintenance".format(
                ", ".join(offline_nodes)
            )
        )
        sys.exit(1)


@cluster.command("pre-flight-check")
@click.pass_context
def preFlightCheck(ctx):
    """
    Check if the environment is ready for upgrade
    """
    return _preFlightCheck(ctx)


@cluster.command(
    "upgrade",
    help="Upgrade cluster, update all nodes and cluster and update kubernetes resources",
)
@click.option("--development", default=False, is_flag=True)
@click.option("--pre-flight-check", default=False, is_flag=True)
@click.pass_context
def upgrade_cluster(ctx, development, pre_flight_check):
    """
    Will upgrade the env.
    """
    ovcversion = ctx.obj["versions"]["ovcversion"]
    timestamp = datetime.utcnow().strftime("%a %b %d %Y %H:%M:%S")
    log("Timestamp: {} UTC".format(timestamp))
    log("Upgrading to version: ({}:{}) ...".format(ovcversion.type, ovcversion.version))

    if pre_flight_check:
        _preFlightCheck(ctx)

    config = ctx.obj["config"]
    cluster = Cluster(config, ctx.obj["versions"], development)
    fs_path = cluster.get_base_image()
    pnodes = get_pnodes(
        config,
        versions=ctx.obj["versions"],
        roles=["cpu", "storage", "controller"],
        enabled_only=True,
        fs_path=fs_path,
    )
    cluster.upgrade()
    try:
        try:
            pnode_action(pnodes, "stop")
            cluster.update()
            pnode_action(pnodes, "upgrade")
        finally:
            pnode_action(pnodes, "start")

        log("Starting migration script ...")
        response = portal_api_call(
            "cloudbroker/grid/runUpgradeScript", ctx.obj["config"]
        )
        if response.status_code != 200:
            raise RuntimeError("%s: \n%s" % (response.content, response.status_code))
        log("Updating environment done.")
    except:
        cluster.revert()
        portal_api_call("cloudbroker/grid/upgradeFailed", ctx.obj["config"])
        logging.error("Error:", exc_info=True)
        log("Failed to update environment.")
    finally:
        cluster.clean_base_image_path(fs_path)


@cluster.command("updatedomain", help="Update the environment certificates and domain")
@click.pass_context
def cluster_updatedomain(ctx):
    config = ctx.obj["config"]
    cluster = Cluster(config, ctx.obj["versions"])
    cluster.verify_certificates()
    cluster.write_certificates()
    cluster.write_system_config()
    cluster.kubectl("delete deploy nginx || true")
    cluster.kubectl("delete deploy portal || true")
    template_path = cluster.scripts_dir + "{}"
    cluster.apply_template(template_path.format("nginx"), "deployment")
    cluster.apply_template(template_path.format("portal"), "deployment")
    cluster.install_teleport()
    pnodes = get_pnodes(config, versions=ctx.obj["versions"], roles=["storage"])
    pnode_action(pnodes, "patch_ovs")


@storage.command("applyconfig", help="Apply storage config")
@click.pass_context
def storage_apply_config(ctx):
    config = ctx.obj["config"]
    pnodes = get_pnodes(config, versions=ctx.obj["versions"], roles=["storage"])
    pnode_action(pnodes, "patch_ovs")


if __name__ == "__main__":
    cli()
