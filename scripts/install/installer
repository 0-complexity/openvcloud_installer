#!/usr/bin/env python3
from js9 import j
import netaddr
import sys
import click
import itertools
import os
import requests
import random
import time
import jsonschema
import concurrent.futures.thread
import logging
import threading

REPO_URL = 'https://github.com/0-complexity/openvcloud_installer'
DEV_REPOS = ['https://github.com/0-complexity/openvcloud', 'https://github.com/jumpscale7/jumpscale_core7', 'https://github.com/jumpscale7/jumpscale_portal']
REPO_PATH = '/opt/code/github/0-complexity/openvcloud_installer'
AYSCONFIG = '''
metadata.jumpscale             =
    branch:'{version}',
    url:'git@github.com:jumpscale7/ays_jumpscale7',

metadata.openvcloud            =
    branch:'{ovcversion}',
    url:'git@github.com:0-complexity/openvcloud_ays',
'''

WHOAMI = '''
email                          =

fullname                       =

git.login                      = ''
git.passwd                     = ''
'''
ERROR_COLOR = u'\u001b[31m'
RESET_COLOR  = u'\u001b[0m'
COLORBLUE = u'\u001b[34m'
COLORGREEN = u'\u001b[32m'
CURSORUP = u'\u001b[{n}A'
CLEAREOL = u'\u001b[0K'
ISINTERACTIVE = sys.stdout.isatty()

status = {}
lock = threading.Lock()
logging.raiseExceptions = False

def log(msg):
    sys.stdout.write(msg + '\n')
    sys.stdout.flush()

def print_status(clear=False):
    with lock:
        if not ISINTERACTIVE:
            return
        if clear:
            sys.stdout.write(CURSORUP.format(n=len(status)))
        length = len(max(status.keys(), key=lambda x: len(x)))
        for node, line in sorted(status.items()):
            line = "{blue}{node:{length}}{reset}: {msg}{eol}".format(
                    blue=COLORBLUE,
                    node=node,
                    length=length,
                    reset=RESET_COLOR,
                    msg=line,
                    eol=CLEAREOL
            )
            log(line)

def get_ipaddress(net):
    return str(netaddr.IPNetwork(net).ip)

def get_pnodes_names(config):
    pnodes_names = []
    for host in config['controller']['hosts']:
        pnodes_names.append(host['hostname'])
    for pnode in itertools.chain(config['nodes']['cpu'], config['nodes']['storage']):
        pnodes_names.append(pnode['name'])
    return pnodes_names

def pnode_action(pnames, action, config, versions):
    futures = {}
    running_futures = {}
    results = ''
    with concurrent.futures.thread.ThreadPoolExecutor(len(pnames)) as proc:
        for pname in pnames:
            status[pname] = 'Started {}'.format(action)

        print("Waiting for {action} node to be done...".format(action=action))
        print_status()
        for pname in pnames:
            jumpscale = PNode(config, pname, versions)
            func = getattr(jumpscale, action)
            fut = proc.submit(func)
            futures[pname] =  fut
            running_futures[pname] = fut

        def update_status():
            messages = ''
            future_error = None
            for name, future in list(running_futures.items()):
                if future.done():
                    if future.exception():
                        messages += '{color}Error when performing node {action} {name}: {reset}{exc} \n'.format(color=ERROR_COLOR, action=action, name=name, reset=RESET_COLOR, exc=str(future.exception()))
                        running_futures.clear()
                        future_error = future.exception()
                    else:
                        status[name] = '{}Finished{}'.format(COLORGREEN, RESET_COLOR)
                        print_status(True)
                        running_futures.pop(name)
            return future_error, messages

        while running_futures:
            time.sleep(5)
            error, msg = update_status()
            results += msg
        print(results)
        if error:
            raise error

def prepare_config(config_path):
    def _helper(nodes):
        for node in nodes:
            net = netaddr.IPNetwork(value['network'])
            ip = net.ip + node['ip-lsb']
            if key not in node:
                node[key] = {}
            node[key]['ipaddress'] = '{ip}/{sub}'.format(ip=ip, sub=net.prefixlen)

    config = j.data.serializer.yaml.load(config_path)
    # add default directories for syncthing
    default_dirs = [
        {
            "path": "/var/ovc/billing",
            "sync": True
        },
        {
            "path": "/var/ovc/influxdb",
            "sync": True
        },
        {
            "path": "/var/ovc/mongodb",
            "sync": False
        },
        {
            "path": "/var/ovc/pxeboot",
            "sync": True
        },
        {
            "path": "/var/ovc/grafana",
            "sync": True
        },
        {
            "path": "/var/ovc/0-access",
            "sync": True
        },
        {
            "path": "/var/ovc/ssl",
            "sync": True
        },
        {
            "path": "/var/ovc/.ssh",
            "sync": True
        },
        {
            "path": "/var/ovc/updatelogs",
            "sync": True
        }
    ]
    config['controller']['directories'] = config['controller'].get('directories', []) + default_dirs
    validator = j.data.serializer.json.load('{}/scripts/kubernetes/config/config-validator.json'.format(REPO_PATH))
    try:
        jsonschema.validate(config, validator)
    except Exception as error:
        message = getattr(error, "message", str(type(error)))
        tree = ''
        for seq in getattr(error, "path", list()):
            if isinstance(seq, int):
                tree += '/<sequence {}>'.format(seq)
            else:
                tree += "/{}".format(seq)

        validator = getattr(error, "validator")
        if  validator == 'type':
            message = '{msg} at {tree}'.format(msg=message, tree=tree)
        elif validator == 'required':
            message = "{msg} in config at {tree}. Please check example config for reference.".format(msg=message, tree=tree)
        raise j.exceptions.RuntimeError(message)

    for key, value in config['network'].items():
        if 'network' in value:
            for nodes in config['nodes'].values():
                _helper(nodes)
            _helper(config['controller']['hosts'])
    cmd = 'echo \'{}\' | ssh-keygen -y -f /dev/stdin'.format(config['ssh']['private-key'])
    _, public_key, _ = j.sal.process.execute(cmd, showout=False)
    config['ssh']['public-key'] = public_key
    j.clients.ssh.start_ssh_agent()
    if j.sal.fs.exists('~/.ssh/id_rsa'):
        j.clients.ssh.load_ssh_key('~/.ssh/id_rsa')
    key = config['ssh']['private-key']
    j.sal.process.execute('echo "%s" | ssh-add /dev/stdin' % key, showout=False)
    return config

def get_versions(version=None, url=None):
    if not url:
        url = 'https://raw.githubusercontent.com/0-complexity/home/master/manifests/{0}.yml'.format(version)
    tmpdir = j.tools.prefab.local.core.file_get_tmp_path()
    j.tools.prefab.local.core.dir_ensure(tmpdir)
    version_path = '{}/versions-manifest.yaml'.format(tmpdir)
    try:
        j.tools.prefab.local.core.file_download(url, version_path, minsizekb=0)
        versions = j.data.serializer.yaml.load(version_path)
        versions['url'] = url
        versions['version'] = version
    finally:
        j.tools.prefab.local.core.dir_remove(tmpdir)

    if 404 in versions:
        raise RuntimeError("Could not find manifest @ {}".format(url))
    for repo in versions['repos']:
        if repo['url'] in "https://github.com/jumpscale7/ays_jumpscale7":
            versions['jsversion'] = repo['target'].get('branch', repo['target'].get('tag', version))
        if repo['url'] in "https://github.com/0-complexity/openvcloud_ays":
            versions['ovcversion'] = repo['target'].get('branch', repo['target'].get('tag', version))
    return versions

def get_controller_management_ips(config):
    for host in config['controller']['hosts']:
        yield get_ipaddress(host['management']['ipaddress'])

def get_controller_public_ips(config):
    for host in config['controller']['hosts']:
        yield get_ipaddress(host['fallback']['ipaddress'])

class AbstractConfig:
    def __init__(self, config, versions=None):
        self.config = config
        self._prefab = None
        self._nodes = None
        self._versions = versions

    @property
    def versions(self):
        if not self._versions:
            error_code, versions, err = self.kubectl('get configmap versions-manifest -o json', die=False)
            if error_code != 0:
                if 'NotFound' in err:
                    raise RuntimeError('configmap minfests does not exist , version needs to be passed during deploy cluster to be registered.')
                raise RuntimeError(err)
            self._versions = j.data.serializer.json.loads(versions)['data']['versions-manifest.yaml']
        return self._versions

    @property
    def prefab(self):
        if self._prefab is None:
            self._prefab = j.tools.prefab.local
        return self._prefab

    @property
    def nodes(self):
        if self._nodes is None:
            nodes = []
            for address in get_controller_public_ips(self.config):
                executor = j.tools.executor.getSSHBased(address, usecache=False)
                nodes.append(j.tools.prefab.get(executor))
            self._nodes = nodes
        return self._nodes

    @property
    def first_node(self):
        return self.nodes[0]

    def kubectl(self, *args, **kwargs):
        cmd = "kubectl "
        cmd += " ".join(args)
        return self.first_node.core.run(cmd, showout=False, **kwargs)

    def get_node_from_appname(self, appname):
        status, output, _ = self.kubectl("get pods -o json")
        data = j.data.serializer.json.loads(output)
        for pod in data['items']:
            if pod['metadata']['labels'].get('app') != appname:
                continue
            hostip = pod['status']['hostIP']
            for node in self.nodes:
                netinfo = node.system.net.getInfo()
                for nic in netinfo:
                    for ip in nic['ip']:
                        if ip == hostip:
                            return node

            raise LookupError("Could not find host with IP {}".format(hostip))
        raise LookupError("Could not find appname {}".format(appname))

    def remove_env_keys(self, node, public_key):
        """
        Remove unwanted public keys from authorized keys under /root/.ssh/authorized_keys.
        """
        auth_keys = node.core.file_read('/root/.ssh/authorized_keys')
        data = []
        for key in auth_keys.splitlines():
            if key.startswith(public_key.strip()):
                continue
            data.append(key)
        data.append('') # end with empty line
        node.core.file_write('/root/.ssh/authorized_keys', '\n'.join(data))

    def add_env_keys(self, node, pub_key, priv_key):
        """
        adds pub and priv key to relative envs
        """
        # create dir
        ovcdir = '/var/ovc/'
        node.core.dir_ensure('{}/.ssh'.format(ovcdir), mode='600')
        node.core.dir_ensure('{}/root/.ssh'.format(ovcdir), mode='600')
        # add keys
        node.executor.sshclient.ssh_authorize('root', pub_key)
        node.core.file_write('{}/.ssh/id_rsa'.format(ovcdir), priv_key, mode='600')
        node.core.file_write('{}/.ssh/id_rsa.pub'.format(ovcdir), pub_key)
        node.core.file_write('{}/.ssh/authorized_keys'.format(ovcdir), pub_key)
        node.core.dir_ensure('/var/ovc/pxeboot/images/', mode='777')
        node.core.file_write('/var/ovc/pxeboot/images/pubkey', pub_key, mode='777')

    def get_new_keys(self):
        privatekey = self.first_node.core.file_read('/var/ovc/.ssh/id_rsa')
        pubkey = self.first_node.core.file_read('/var/ovc/.ssh/id_rsa.pub')
        return privatekey, pubkey

    def configure_keys(self):
        log('Configuring new keys')
        if not len(self.nodes):
            raise RuntimeError('cluster needs to be depployed before configuring keys')

        #get old keys
        old_pub_key = self.config['ssh']['public-key']
        # generate keys
        public_key_path = self.first_node.system.ssh.keygen(name='id_rsa').strip()
        private_key_path = public_key_path[:-4]

        # add keys to kuberntes
        public_key = self.first_node.core.file_read(public_key_path)
        private_key = self.first_node.core.file_read(private_key_path)

        # add keys to controller nodes
        for node in self.nodes:
            self.add_env_keys(node, public_key, private_key)
            self.remove_env_keys(node, old_pub_key)


class Node(AbstractConfig):
    def __init__(self, config, name, versions):
        super().__init__(config, versions)
        self.name = name
        self._node = None
        self._nodetype = None
        self.managementip = get_ipaddress(self.node['management']['ipaddress'])
        self._prefab = None
        self.config = config
        self.password = config['environment']['password']
        self.gid = config['environment']['grid']['id']
        self.managementips = list(get_controller_management_ips(config))
        self.fqdn = '{}.{}'.format(config['environment']['subdomain'], config['environment']['basedomain'])
        self.iyourl = 'https://itsyou.online/'

    def log(self, msg):
        if ISINTERACTIVE and self.name in status:
            status[self.name] = msg
            print_status(True)
        else:
            msg = "{}: {}".format(self.name, msg)
            log(msg)

    @property
    def prefab(self):
        if self._prefab is None:
            self._prefab = j.tools.prefab.getFromSSH(self.managementip)
        return self._prefab

    @property
    def node(self):
        if self._node is None:
            for nodetype in ('cpu', 'storage'):
                for node in self.config['nodes'][nodetype]:
                    if node['name'] == self.name:
                        self._nodetype = nodetype
                        self._node = node
                        return self._node
            else:
                for node in self.config['controller']['hosts']:
                    if node['hostname'] == self.name:
                        self._node = node
                        self._nodetype = 'controller'
                        break
                else:
                    raise LookupError("Failed to find node with name {}".format(self.name))
        return self._node

    @property
    def nodetype(self):
        if self._nodetype is None:
            self.node
        return self._nodetype


class JumpScale7(Node):
    def install_core(self):
        jsversion = self.versions['jsversion']
        ovcversion = self.versions['ovcversion']
        env = {'AYSBRANCH': jsversion, 'JSBRANCH': jsversion}
        cmd = 'cd /tmp;rm -f install.sh;curl -k https://raw.githubusercontent.com/jumpscale7/jumpscale_core7/{}/install/install.sh > install.sh;bash install.sh'.format(jsversion)
        self.prefab.system.ssh.define_host('git.aydo.com')
        self.prefab.system.ssh.define_host('github.com')
        self.prefab.system.ssh.define_host('docs.greenitglobe.com', port=10022)

        if self.prefab.bash.cmdGetPath('python', False) is False or self.prefab.bash.cmdGetPath('curl', False) is False:
            self.prefab.system.package.mdupdate()
            self.prefab.system.package.install('python')
            self.prefab.system.package.install('curl')
        if self.prefab.bash.cmdGetPath('js', False) is False:
            self.prefab.core.run(cmd, env=env)
        self.prefab.core.file_write('/opt/jumpscale7/hrd/system/atyourservice.hrd', AYSCONFIG.format(version=jsversion, ovcversion=ovcversion))
        self.prefab.core.file_write('/opt/jumpscale7/hrd/system/whoami.hrd', WHOAMI)

    def start(self):
        self.log("Start JumpScale services")
        self.prefab.core.run('ays start')

    def stop(self):
        self.log("Stop JumpScale services")
        self.prefab.core.run('ays stop')
        self.prefab.core.run('fuser -k 4446/tcp || true')

    def update_repo(self, account, repo, version):
        cmd = "jscode update -a '%s' -n '%s' -d -b %s --https" % (account, repo, version)
        self.prefab.core.run(cmd)


    def update(self):
        js_version = self.versions['jsversion']
        ovc_version = self.versions['ovcversion']
        self.log("Updating JumpScale repos")
        self.update_repo('jumpscale7', '*', js_version)
        self.log("Updating OpenvCloud repos")
        self.update_repo('0-complexity', 'openvcloud', ovc_version)
        self.update_repo('0-complexity', 'openvcloud_installer', ovc_version)

    def restart(self):
        self.stop()
        self.start()

    def install_agent(self, roles=None):
        roles = roles or ['node']
        redisdata = {
            'instance.param.disk': '0',
            'instance.param.mem': '100',
            'instance.param.passwd': '',
            'instance.param.port': '9999',
            'instance.param.ip' : '0.0.0.0',
            'instance.param.unixsocket': '0'
        }
        self.ays_install('redis', instance='system', data=redisdata)
        masterips = ','.join(self.managementips)

        osisclientdata = {
            'param.osis.client.addr': masterips,
            'param.osis.client.login': 'root',
            'param.osis.client.passwd': self.password,
            'param.osis.client.port': '5544',
        }
        self.ays_install('osis_client', instance='main', data=osisclientdata)
        self.ays_install('osis_client', instance='jsagent', data=osisclientdata)

        agentcontrollerdata = {
            'agentcontroller.client.addr': masterips,
            'agentcontroller.client.login': 'node',
            'agentcontroller.client.passwd': '',
            'agentcontroller.client.port': '4444'
        }
        self.ays_install('agentcontroller_client', instance='main', data=agentcontrollerdata)

        agentdata = {
                'agentcontroller.connection': 'main',
                'grid.id': str(self.gid),
                'grid.node.roles': ','.join(roles),
                'osis.connection': 'jsagent',
        }
        self.ays_install('jsagent', instance='main', data=agentdata)


    def ays_install(self, package, domain='jumpscale', instance='main', data=None):
        self.log("Installing JumpScale package {}:{}".format(domain, package))
        datastr = ''
        data = data or {}
        for key, value in data.items():
            datastr += "{}:{} ".format(key, value)
        cmd = 'ays install -d {} -n {} -i {} --data "{}"'.format(domain, package, instance, datastr)
        self.prefab.core.run(cmd)

    def install(self):
        if self.nodetype == 'cpu':
            self.install_compute_node()
        elif self.nodetype == 'storage':
            self.install_storage_node()
        elif self.nodetype == 'controller':
            self.install_controller()

    def install_controller(self):
        self.install_core()
        self.install_agent(['node', 'controllernode'])

    def install_compute_node(self):
        self.install_core()
        self.install_agent()
        netinfo = {
            'vxbackend_vlan': self.config['network']['vxbackend']['vlan'],
            'vxbackend_ip': self.node['vxbackend']['ipaddress'],
            'gwmgmt_vlan': self.config['network']['gateway-management']['vlan'],
            'gwmgmt_ip': self.node['gateway-management']['ipaddress'],

        }
        data_net = {
            'netconfig.public_backplane.interfacename': 'backplane1',
            'netconfig.gw_mgmt_backplane.interfacename': 'backplane1',
            'netconfig.vxbackend.interfacename': 'backplane1',
            'netconfig.gw_mgmt.vlanid': netinfo['gwmgmt_vlan'],
            'netconfig.vxbackend.vlanid': netinfo['vxbackend_vlan'],
            'netconfig.gw_mgmt.ipaddr': netinfo['gwmgmt_ip'],
            'netconfig.vxbackend.ipaddr': netinfo['vxbackend_ip'],
        }

        data_cpu = {
            'instance.param.rootpasswd': self.password,
            'instance.param.master.addr': '',
            'instance.param.network.gw_mgmt_ip': netinfo['gwmgmt_ip'],
            'instance.param.grid.id': self.gid,
        }

        self.install_portal_client()
        self.ays_install('scaleout_networkconfig', domain='openvcloud', instance='main', data=data_net)
        self.ays_install('cb_cpunode_aio', domain='openvcloud', instance='main', data=data_cpu)


    def install_portal_client(self):
        portal_client = {
            'param.addr' : self.fqdn,
            'param.port': '443',
            'param.secret': self.password,
        }

        self.ays_install('portal_client', domain='jumpscale', instance='main', data=portal_client)

    def configure_iyo_api_key(self, apikey):
        # TODO: check if we can do this in js9
        label = apikey['label']
        clientid = self.config['itsyouonline']['clientId']
        secret = self.config['itsyouonline']['clientSecret']
        accesstokenparams = {'grant_type': 'client_credentials', 'client_id': clientid, 'client_secret': secret}
        accesstoken = requests.post(os.path.join(self.iyourl, 'v1', 'oauth', 'access_token'), params=accesstokenparams)
        token = accesstoken.json()['access_token']
        authheaders = {'Authorization': 'token %s' % token}
        result = requests.get(os.path.join(self.iyourl, 'api', 'organizations', clientid,
                                           'apikeys', label), headers=authheaders)
        if result.status_code == 200:
            stored_apikey = result.json()
            if apikey['callbackURL'] != stored_apikey['callbackURL']:
                requests.delete(os.path.join(self.iyourl, 'api', 'organizations', clientid,
                                             'apikeys', label), headers=authheaders)
                apikey = {}
            else:
                apikey = stored_apikey

        if 'secret' not in apikey:
            result = requests.post(os.path.join(self.iyourl, 'api', 'organizations',
                                                clientid, 'apikeys'), json=apikey, headers=authheaders)
            apikey = result.json()
        return apikey

    def install_storage_node(self):
        self.install_core()
        self.install_agent()
        self.install_portal_client()

        data_storage = {
            'param.rootpasswd': self.password,
            'param.master.addr': '',
            'param.grid.id': self.gid,
        }

        self.ays_install('cb_storagenode_aio', domain='openvcloud', instance='main', data=data_storage)
        self.ays_install('cb_storagedriver_aio', domain='openvcloud', instance='main', data=data_storage)
        status, output, _ = self.prefab.core.run('ovs config get "ovs/framework/hosts/$(cat /etc/openvstorage_id)/type"')
        if 'MASTER' in output:
            self.prefab.core.run("python /opt/code/github/0-complexity/openvcloud/scripts/ovs/alba-create-user.py")
            ovscallbackurl = 'https://ovs-{}/api/oauth2/redirect/'.format(self.fqdn)
            apikey = {
                'label': 'ovs-{}'.format(self.config['environment']['subdomain']),
                'clientCredentialsGrantType': False,
                'callbackURL' : ovscallbackurl
            }
            apikey = self.configure_iyo_api_key(apikey)

            oauth_token_uri = os.path.join(self.iyourl, 'v1/oauth/access_token')
            oauth_authorize_uri = os.path.join(self.iyourl, 'v1/oauth/authorize')

            data_oauth = {'instance.oauth.id': self.config['itsyouonline']['clientId'],
                        'instance.oauth.secret': apikey['secret'],
                        'instance.oauth.authorize_uri': oauth_authorize_uri,
                        'instance.oauth.token_uri': oauth_token_uri}
            self.ays_install('openvstorage_oauth', 'openvcloud', instance='main', data=data_oauth)



class Cluster(AbstractConfig):
    """
    Cluster abstraction layer to allow for easier manipulation.
    """
    def __init__(self, config_path, versions, development=False):
        super().__init__(config_path, versions)
        self.k8s_config = '/root/.kube/{}.conf'.format(self.config['environment']['subdomain'])
        self.join_line = ''
        self.scripts_dir = '{}/kuberesources/'.format(REPO_PATH)
        self.development = development

    def install_kubernetes_cluster(self):
        """
        Will install a kubernetes master and minion nodes on the first and rest of the node list respectively.
        """
        log('Installing k8 cluster')
        for node in self.nodes:
            node.core.run("sudo sed -i.bak '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab")

        managementips = list(get_controller_management_ips(self.config))
        k8s_config_data, self.join_line = self.prefab.virtualization.kubernetes.multihost_install(
                self.nodes, unsafe=True, reset=True, external_ips=managementips)
        self.prefab.core.dir_ensure('/root/.kube')
        self.prefab.core.file_write(self.k8s_config, k8s_config_data)
        if not self.prefab.core.file_exists('/root/.kube/config'):
            self.prefab.core.file_write('/root/.kube/config', k8s_config_data)

    def clone_repo(self, url=REPO_URL):
        for node in self.nodes:
            node.tools.git.pullRepo(url)

    def _install_kubectl(self, prefab):
        if not prefab.core.file_exists('/usr/local/bin/kubectl'):
            prefab.virtualization.kubernetes.install_kube_client(location='/usr/local/bin/kubectl')

    def verify_certificates(self):
        for key, value in self.config['environment']['ssl'].items():
            for extension in ('crt', 'csr', 'key'):
                path = os.path.join(value['path'], '{}.{}'.format(value['name'], extension))
                if not j.sal.fs.exists(path):
                    raise RuntimeError("Certificate {} does not exists.".format(path))

    def install_controller(self):
        """
        Will use existing yaml or config scripts in this dir as well as jumpscale modules to install the controller setup on the
        cluster. Creating the relevant deployments, services, and mounts
        """
        log('Install kubectl')
        for node in self.nodes:
            self._install_kubectl(node)
        self._install_kubectl(self.prefab)
        self.clone_repo()

        log('Preparing directories')
        directories = self.config['controller'].get('directories')
        for node in self.nodes:
            for directory in directories:
                node.core.dir_ensure(directory['path'], mode='777')

        log('Writing certificates')
        copiedpaths = {}
        ssldir = '/var/ovc/ssl/'
        for key, value in self.config['environment']['ssl'].items():
            path = os.path.join(value['path'], '')
            if path not in copiedpaths:
                for node in self.nodes:
                    node.core.upload(path, ssldir)
                copiedpaths[path] = ssldir

        self.install_teleport()
        self.configure_keys()
        self.kube_client_apply()

    def apply_template(self, template, kind):
        self.kubectl('apply -f {template}'.format(template=template))
        template_name = j.sal.fs.getBaseName(template)
        timeout = time.time() + 240
        print('Waiting for {} to be ready..'.format(template_name))
        while time.time() < timeout:
            template_basename = template.split('/')[-1]
            template_name = os.path.splitext(template_basename)[0] if template_basename.endswith('.yaml') else template_basename
            _, out, _ = self.kubectl('get %s %s -o json' % (kind, template_name))
            json_out = j.data.serializer.json.loads(out)
            ready = False
            if kind == 'deployment':
                for condition in  json_out['status'].get('conditions'):
                    if condition['type'] == 'Available' and condition['status'] == 'True':
                        ready = True
                        break
            if kind == 'statefulset':
                if json_out['status']['replicas'] == json_out['status'].get('readyReplicas'):
                    ready = True
                    break
            if ready:
                break
        else:
            raise j.exceptions.Timeout('Deploying {} took longer than expected. Exiting..'.format(template_name))

    def dump(self, location, data, dumper=j.data.serializer.yaml.dumps):
        data = dumper(data)
        self.first_node.core.file_write(location, data)

    def load(self, location, loader=j.data.serializer.yaml.loads):
        data = self.first_node.core.file_read(location)
        return loader(data)

    def write_service_template(self, template, externalips):
        template_loc = '{dir}{name}/{name}-service.yaml'.format(dir=self.scripts_dir, name=template)
        data = self.load(template_loc)
        if data['spec'].get('type') == 'NodePort':
            data['spec']['externalIPs'] = externalips
            self.dump(template_loc, data)
        return template_loc

    def _apply_nginx(self, name):
        pubips = list(get_controller_public_ips(self.config))
        loc = self.write_service_template(name, pubips)
        self.kubectl('apply -f {}'.format(loc))

    def upgrade(self):
        self._apply_nginx('upgrader')

    def revert(self):
        """
        Reconfigure nginx service to working state
        """
        self._apply_nginx('nginx')

    def prepare_templates(self):
        log('Preparing resources')
        def write_deployment_template(template):
            template_loc = '{dir}{name}/{name}-deployment.yaml'.format(dir=self.scripts_dir, name=template)
            data = self.load(template_loc)
            for container in data['spec']['template']['spec']['containers']:
                volmounts = container.setdefault('volumeMounts', [])
                volmounts.append({'name': 'code', 'mountPath': '/opt/code/github/0-complexity', 'subPath': 'github/0-complexity'})
                volmounts.append({'name': 'code', 'mountPath': '/opt/code/github/jumpscale7', 'subPath': 'github/jumpscale7'})
            volumes = data['spec']['template']['spec'].setdefault('volumes', [])
            volumes.append({'name': 'code', 'hostPath': {'path': '/opt/code', 'type': 'Directory'}})
            self.dump(template_loc, data)

        self.first_node.core.upload('{}/scripts/kubernetes/'.format(REPO_PATH), self.scripts_dir)

        # have to use ls as it is a remote directory i need to traverse
        def change_container_images(template):
            types = ('Deployment', 'StatefulSet', 'Job')
            data = self.load(template)
            if isinstance(data, dict) and data.get('kind') in types:
                for conttype  in ('initContainers', 'containers'):
                    if conttype not in data['spec']['template']['spec']:
                        continue
                    for container in data['spec']['template']['spec'][conttype]:
                        #get image version from the self.versions dict loaded from 0-complexity/home
                        if container['image'] in self.versions['images']:
                            container['image'] = "{}:{}".format(container['image'], self.versions['images'][container['image']])
                self.dump(template, data)

        # add external ips to the specified services to expose to the internal network
        templates = ['agentcontroller', 'osis']
        externalips = list(get_controller_management_ips(self.config))
        for template in templates:
            self.write_service_template(template, externalips)
        # add external ips to the specified services to expose to the internet
        pubtemplates = ['nginx', 'zero-access', 'management']
        pubips = list(get_controller_public_ips(self.config))
        for template in pubtemplates:
            self.write_service_template(template, pubips)
        # add code to mount specdied repos for in developmnent mode
        if self.development:
            for template in ['osis', 'agentcontroller', 'portal']:
                write_deployment_template(template)
            for repo in DEV_REPOS:
                self.clone_repo(repo)

        for template in self.first_node.core.find(self.scripts_dir, pattern='*.yaml'):
            change_container_images(template)

        stat_template = self.scripts_dir + '/stats-collector/stats-deployment.yaml'
        data = self.load(stat_template)
        data['spec']['template']['spec']['containers'][0]['args'][4] = self.config['network']['management']['network']
        self.dump(stat_template, data)

    def write_config_map(self, name, content):
        if isinstance(content, (dict, list)):
            content = j.data.serializer.yaml.dumps(content)
        self.first_node.core.run("cat << EOF | kubectl create configmap {0} --dry-run --from-file={0}.yaml=/dev/stdin -o json | kubectl apply -f -\n{1}\nEOF".format(name, content))

    def write_system_config(self):
        log('Write system config')
        self.write_config_map('system-config', self.config)
        self.write_config_map('versions-manifest', self.versions)

    def install_teleport(self):
        for node in self.nodes:
            log('Installing teleport {}'.format(node.core.hostname))
            ssl_name = self.config['environment']['ssl']['root']['name']
            if not node.core.file_exists('/usr/local/bin/teleport') or not node.core.file_exists('/usr/local/bin/tctl') or not node.core.file_exists('/usr/local/bin/tsh'):
                node.apps.teleport.package_install(extra_paths=['/usr/local/bin'])
            node.apps.teleport.write_config(name=node.core.hostname, key_path='/var/ovc/ssl/%s.key' % ssl_name,
                                            cert_path='/var/ovc/ssl/%s.crt' % ssl_name)
            node.apps.teleport.restart()
            github = self.config['support']['github']
            teams = ['/'.join([team['org_name'], team['team_name']]) for team in github['teams']]
            _, resources, _ = node.core.run('tctl get github/%s' % node.core.hostname)
            if node.core.hostname in resources:
                return
            node.apps.teleport.apply_permissions(
                name=node.core.hostname,
                client_id=github['client_id'],
                client_secret=github['client_secret'],
                teams=teams,
                exposed_ip='{}.{}'.format(self.config['environment']['subdomain'], self.config['environment']['basedomain'])
            )

    def kube_client_apply(self):
        self.prepare_templates()
        self.write_system_config()

        self.kubectl('apply -f {path}/rbac.yaml'.format(path=self.scripts_dir))
        deployments = ['influxdb', 'osis', 'agentcontroller', 'stats-collector',
                       'portal', 'pxeboot', 'zero-access', 'management', 'ovs', 'controller-jsagent', 'nginx']
        statefulSets = ['mongo', 'syncthing']
        for statefulSet in statefulSets:
            template_file = self.scripts_dir + statefulSet
            self.apply_template(template_file, kind='statefulset')
        for deployment in deployments:
            template_file = self.scripts_dir + deployment
            self.apply_template(template_file, kind='deployment')

        self.grafana_apply('{}/grafana'.format(self.scripts_dir))

    def grafana_apply(self, grafana_dir):
        log('Installing grafana')
        datasourcepath = '{}/provisioning/datasources/influx.yaml'.format(grafana_dir)
        dashboardpath = '{}/sources/templates'.format(grafana_dir)
        datasource = self.load(datasourcepath)
        datasourcename = 'controller_{}'.format(self.config['environment']['subdomain'])
        datasource['datasources'][0]['name'] = datasourcename
        self.dump(datasourcepath, datasource)
        for dashboardfile in self.first_node.core.find(dashboardpath, pattern='*.json'):
            db = self.load(dashboardfile, j.data.serializer.json.loads)
            db['id'] = None
            db['title'] += " ({})".format(self.config['environment']['subdomain'])
            for row in db['rows']:
                for panel in row['panels']:
                    panel['datasource'] = datasourcename
            if 'templating' in db:
                for item in db['templating']['list']:
                    item['datasource'] = datasourcename
            self.dump(dashboardfile, db, j.data.serializer.json.dumps)
        cmd = """
        cd {dir}
        kubectl create configmap grafana-provisioning-datasources --from-file=provisioning/datasources
        kubectl create configmap grafana-provisioning-dashboards --from-file=provisioning/dashboards
        kubectl create configmap grafana-dashboards --from-file=sources/templates
        kubectl apply -f grafana-service.yaml
        kubectl apply -f grafana-deployment.yaml
        """.format(config=self.k8s_config, dir=grafana_dir)
        self.first_node.core.execute_bash(cmd)

    def delete(self):
        log('Deleting configmaps')
        self.kubectl('delete configmap --all --namespace default')
        log('Deleting services')
        self.kubectl("delete service --namespace default -l 'name != nginx'")
        log('Deleting statefulset')
        self.kubectl('delete statefulset --all --namespace default')
        log('Deleting deployments')
        self.kubectl('delete deploy --all --namespace default')

    def update(self):
        log('Restarting cluster...')
        self.delete()
        self.kube_client_apply()

class PNode(JumpScale7):
    def ipmi(self, *args):
        ipmiip = get_ipaddress(self.node['ipmi']['ipaddress'])
        cmd = "ipmitool -I lanplus -H {} -U {} -P {} ".format(ipmiip, self.node['ipmi']['username'], self.node['ipmi']['password'])
        cmd += " ".join(args)
        self.first_node.core.run(cmd)

    def reboot(self):
        self.log('Rebooting')
        self.ipmi('chassis power cycle')

    def is_up(self):
        return j.sal.nettools.tcpPortConnectionTest(self.managementip, 22)

    def wait_up(self, timeout=600):
        j.sal.nettools.waitConnectionTest(self.managementip, 22, timeout)
        if not self.is_up():
            raise RuntimeError('Node did not come up in {}seconds'.format(timeout))

    def enable_pxe(self):
        self.log('Enable PXE')
        prefab = self.get_node_from_appname('pxeboot')
        self.ipmi('chassis bootdev pxe')
        macaddress = str(netaddr.EUI(self.node['management']['macaddress'])).lower()
        dst = '/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}'.format(macaddress)
        src = '../../conf/tftp-911boot'
        prefab.core.file_link(src, dst)

    def disable_pxe(self):
        self.log('Disable PXE')
        prefab = self.get_node_from_appname('pxeboot')
        macaddress = str(netaddr.EUI(self.node['management']['macaddress'])).lower()
        dst = '/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}'.format(macaddress)
        prefab.core.file_unlink(dst)

    def install_os(self):
        self.enable_pxe()
        self.reboot()
        time.sleep(20)
        self.log('Waiting for 911 boot')
        self.wait_up()
        self.log('Installing OS')
        status, output, err = self.prefab.core.run('cd /root/tools/; ./Install')
        if status != 0:
            raise RuntimeError('Failed to install OS')
        self.disable_pxe()
        self.reboot()
        time.sleep(20)
        self.log('Waiting for OS boot')
        self.wait_up()

###                    ###
# Command line interface #
###                    ###

@click.group()
@click.option('--config', help='Config file to deploy the cluster', envvar='ENV_CONFIG')
@click.option('--version', help='version number to install or update to', envvar='OVC_VERSION', default=None)
@click.option('--url', help='version url to install from', envvar='OVC_VERSION_URL', default=None)
@click.option('--loglevel', default=50, help='Set loglevel', type=int)
@click.pass_context
def cli(ctx, config, version, url, loglevel):
    j.logger.set_level(loglevel)
    ctx.obj = {}
    if not config:
        raise j.exceptions.Input('Please specify a config file')
    config = prepare_config(config)
    ctx.obj['config'] = config
    ctx.obj['versions'] = get_versions(version, url) if (version or url) else None

@cli.group()
def cluster():
    pass

@cluster.group(help='Manipulation of kubernetes resources')
def resources():
    pass

@cli.group()
def node(help='Node actions'):
    pass

@cli.group()
def image(help='Image actions'):
    pass


@image.command('deploy', help='Deploy an VM Image')
@click.option('--name', help='Image name')
@click.pass_context
def image_deploy(ctx, name):
    storage = random.choice(ctx.obj['config']['nodes']['storage'])
    jumpscale = JumpScale7(ctx.obj['config'], storage['name'], ctx.obj['versions'])
    jumpscale.ays_install(name, 'openvcloud')


@node.command('action')
@click.option('--name', help='Node name pass all to apply on all comma seperate list for multiple')
@click.argument('action', type=click.Choice(['start', 'stop', 'update', 'restart', 'install', 'reboot', 'is_up', 'wait_up', 'enable_pxe', 'disable_pxe', 'install_os']))
@click.pass_context
def node_action(ctx, name, action):
    """
    Execute command for node
    """
    if name != 'all' and ',' not in name:
        cluster = PNode(ctx.obj['config'], name, ctx.obj['versions'])
        getattr(cluster, action)()
    else:
        if name == 'all':
            pnames = [pnode['name'] for pnode in itertools.chain(ctx.obj['config']['nodes']['cpu'], ctx.obj['config']['nodes']['storage'])]
        else:
            pnames = name.split(',')
        pnode_action(pnames, action, ctx.obj['config'], versions=ctx.obj['versions'])

@node.command('update', help='Update code and restart required services on node')
@click.pass_context
def node_update(ctx):
    return _node_update(ctx)

def _node_update(ctx):
    """
    Update all pnodes
    """
    config = ctx.obj['config']
    pnames = get_pnodes_names(config)
    pnode_action(pnames, 'update', config, versions=ctx.obj['versions'])
    pnode_action(pnames, 'restart', config, versions=ctx.obj['versions'])

@resources.command('writeconfig', help='Write system-config to kubernetes')
@click.pass_context
def write_system_config(ctx):
    """
    Write or update system-config in kubernetes configmap based on yaml file
    """
    cluster = Cluster(ctx.obj['config'], ctx.obj['versions'])
    cluster.write_system_config()


@resources.command('write', help='Rewrite all templates')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def write_kube_resources(ctx, development):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj['config'], ctx.obj['versions'], development)
    cluster.prepare_templates()


@resources.command('applyall', help='Apply all kubernetes resources')
@click.pass_context
def kube_apply_all(ctx):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj['config'], ctx.obj['vesions'])
    cluster.kube_client_apply()


@resources.command('apply', help='Apply a specifified kubernetes resource')
@click.option('--path', help='path to template')
@click.pass_context
def kube_apply(ctx, path):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj['config'], ctx.obj['versions'])
    cluster.apply_template(path, kind='deployment')

@cluster.command('deploy', help='Deploy kubernetes cluster on controller nodes')
@click.option('--configure-cluster/--no-configure-cluster', help='Configure kubernetes cluster', default=True)
@click.pass_context
def deploy_cluster(ctx, configure_cluster):
    """
    Deploy will create the cluster machines and deploy kubernetes cluster on top of them.
    """
    cluster = Cluster(ctx.obj['config'], ctx.obj['versions'])
    cluster.verify_certificates()
    if configure_cluster:
        cluster.install_kubernetes_cluster()
    cluster.install_controller()

@cluster.command('update', help='Update cluster, delete existing resources and reapply')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def update_cluster(ctx, development):
    return _update_cluster(ctx, development)

def _update_cluster(ctx, development):
    """
    Will update the cluster.
    """
    cluster = Cluster(ctx.obj['config'], ctx.obj['versions'], development)
    cluster.update()

@cluster.command('upgrade', help='Upgrade cluster, update all nodes and cluster and update kubernetes resources')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def upgrade_cluster(ctx, development):
    """
    Will upgrade the env.
    """
    if not development:
        j.logger.set_level(30)
    config = ctx.obj['config']
    cluster = Cluster(config, ctx.obj['versions'], development)
    cluster.upgrade()
    try:
        pnames = get_pnodes_names(config)
        pnode_action(pnames, 'stop', config, versions=ctx.obj['versions'])
        pnode_action(pnames, 'update', config, versions=ctx.obj['versions'])
        cluster.update()
        pnode_action(pnames, 'start', config, versions=ctx.obj['versions'])
        print('Updating env done.')
    except:
        cluster.revert()
        print("Failed to update env.")

if __name__ == '__main__':
    cli()

