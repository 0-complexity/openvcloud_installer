#!/usr/bin/env python3
from js9 import j
import netaddr
import sys
import click
import itertools
import os
import requests
import random
import time
import jsonschema
import concurrent.futures.thread
import logging
import threading

REPO_URL = 'https://github.com/0-complexity/openvcloud_installer'
DEV_REPOS = ['https://github.com/0-complexity/openvcloud', 'https://github.com/jumpscale7/jumpscale_core7', 'https://github.com/jumpscale7/jumpscale_portal']
REPO_PATH = '/opt/code/github/0-complexity/openvcloud_installer'
AYSCONFIG = '''
metadata.jumpscale             =
    branch:'{version}',
    url:'git@github.com:jumpscale7/ays_jumpscale7',

metadata.openvcloud            =
    branch:'{ovcversion}',
    url:'git@github.com:0-complexity/openvcloud_ays',
'''

WHOAMI = '''
email                          =

fullname                       =

git.login                      = ''
git.passwd                     = ''
'''
ERROR_COLOR = u'\u001b[31m'
RESET_COLOR  = u'\u001b[0m'
COLORBLUE = u'\u001b[34m'
COLORGREEN = u'\u001b[32m'
CURSORUP = u'\u001b[{n}A'
CLEAREOL = u'\u001b[0K'
ISINTERACTIVE = sys.stdout.isatty()

status = {}
lock = threading.Lock()
logging.raiseExceptions = False

def log(msg):
    sys.stdout.write(msg + '\n')
    sys.stdout.flush()

def print_status(clear=False):
    with lock:
        if not ISINTERACTIVE:
            return
        if clear:
            sys.stdout.write(CURSORUP.format(n=len(status)))
        length = len(max(status.keys(), key=lambda x: len(x)))
        for node, line in sorted(status.items()):
            line = "{blue}{node:{length}}{reset}: {msg}{eol}".format(
                    blue=COLORBLUE,
                    node=node,
                    length=length,
                    reset=RESET_COLOR,
                    msg=line,
                    eol=CLEAREOL
            )
            log(line)

def get_ipaddress(net):
    return str(netaddr.IPNetwork(net).ip)

def get_pnodes_names(config):
    pnodes_names = []
    for host in config['controller']['hosts']:
        pnodes_names.append(host['hostname'])
    for pnode in itertools.chain(config['nodes']['cpu'], config['nodes']['storage']):
        pnodes_names.append(pnode['name'])
    return pnodes_names

def pnode_action(pnames, action, config):
    futures = {}
    running_futures = {}
    results = ''
    with concurrent.futures.thread.ThreadPoolExecutor(len(pnames)) as proc:
        for pname in pnames:
            status[pname] = 'Started {}'.format(action)

        print("Waiting for {action} node to be done...".format(action=action))
        print_status()
        for pname in pnames:
            jumpscale = PNode(config, pname)
            func = getattr(jumpscale, action)
            fut = proc.submit(func)
            futures[pname] =  fut
            running_futures[pname] = fut

        def update_status():
            messages = ''
            future_error = None
            for name, future in list(running_futures.items()):
                if future.done():
                    if future.exception():
                        messages += '{color}Error when performing node {action} {name}: {reset}{exc} \n'.format(color=ERROR_COLOR, action=action, name=name, reset=RESET_COLOR, exc=str(future.exception()))
                        running_futures.clear()
                        future_error = future.exception()
                    else:
                        status[name] = '{}Finished{}'.format(COLORGREEN, RESET_COLOR)
                        print_status(True)
                        running_futures.pop(name)
            return future_error, messages

        while running_futures:
            time.sleep(5)
            error, msg = update_status()
            results += msg
        print(results)
        if error:
            raise error

def prepare_config(config_path):
    def _helper(nodes):
        for node in nodes:
            net = netaddr.IPNetwork(value['network'])
            ip = net.ip + node['ip-lsb']
            if key not in node:
                node[key] = {}
            node[key]['ipaddress'] = '{ip}/{sub}'.format(ip=ip, sub=net.prefixlen)

    config = j.data.serializer.yaml.load(config_path)
    # add default directories for syncthing
    default_dirs = [
        {
            "path": "/var/ovc/billing",
            "sync": True
        },
        {
            "path": "/var/ovc/influxdb",
            "sync": True
        },
        {
            "path": "/var/ovc/mongodb",
            "sync": False
        },
        {
            "path": "/var/ovc/pxeboot",
            "sync": True
        },
        {
            "path": "/var/ovc/0-access",
            "sync": True
        },
        {
            "path": "/var/ovc/ssl",
            "sync": True
        },
        {
            "path": "/var/ovc/.ssh",
            "sync": True
        },
        {
            "path": "/var/ovc/updatelogs",
            "sync": True
        }
    ]
    config['controller']['directories'] = config['controller'].get('directories', []) + default_dirs
    validator = j.data.serializer.json.load('{}/scripts/kubernetes/config/config-validator.json'.format(REPO_PATH))
    try:
        jsonschema.validate(config, validator)
    except Exception as error:
        message = getattr(error, "message", str(type(error)))
        tree = ''
        for seq in getattr(error, "path", list()):
            if isinstance(seq, int):
                tree += '/<sequence {}>'.format(seq)
            else:
                tree += "/{}".format(seq)

        validator = getattr(error, "validator")
        if  validator == 'type':
            message = '{msg} at {tree}'.format(msg=message, tree=tree)
        elif validator == 'required':
            message = "{msg} in config at {tree}. Please check example config for reference.".format(msg=message, tree=tree)
        raise j.exceptions.RuntimeError(message)

    for key, value in config['network'].items():
        if 'network' in value:
            for nodes in config['nodes'].values():
                _helper(nodes)
            _helper(config['controller']['hosts'])
    cmd = 'echo \'{}\' | ssh-keygen -y -f /dev/stdin'.format(config['ssh']['private-key'])
    _, public_key, _ = j.sal.process.execute(cmd, showout=False)
    config['ssh']['public-key'] = public_key
    j.clients.ssh.start_ssh_agent()
    if j.sal.fs.exists('~/.ssh/id_rsa'):
        j.clients.ssh.load_ssh_key('~/.ssh/id_rsa')
    key = config['ssh']['private-key']
    j.sal.process.execute('echo "%s" | ssh-add /dev/stdin' % key, showout=False)
    return config

def get_controller_management_ips(config):
    for host in config['controller']['hosts']:
        yield get_ipaddress(host['management']['ipaddress'])

def get_controller_public_ips(config):
    for host in config['controller']['hosts']:
        yield get_ipaddress(host['fallback']['ipaddress'])

class AbstractConfig:
    def __init__(self, config):
        self.config = config
        self._prefab = None
        self._nodes = None

    @property
    def prefab(self):
        if self._prefab is None:
            self._prefab = j.tools.prefab.local
        return self._prefab

    @property
    def nodes(self):
        if self._nodes is None:
            nodes = []
            for address in get_controller_public_ips(self.config):
                executor = j.tools.executor.getSSHBased(address, usecache=False)
                nodes.append(j.tools.prefab.get(executor))
            self._nodes = nodes
        return self._nodes

    @property
    def first_node(self):
        return self.nodes[0]

    def kubectl(self, *args, **kwargs):
        cmd = "kubectl "
        cmd += " ".join(args)
        return self.first_node.core.run(cmd, showout=False, **kwargs)

    def get_node_from_appname(self, appname):
        status, output, _ = self.kubectl("get pods -o json")
        data = j.data.serializer.json.loads(output)
        for pod in data['items']:
            if pod['metadata']['labels'].get('app') != appname:
                continue
            hostip = pod['status']['hostIP']
            for node in self.nodes:
                netinfo = node.system.net.getInfo()
                for nic in netinfo:
                    for ip in nic['ip']:
                        if ip == hostip:
                            return node

            raise LookupError("Could not find host with IP {}".format(hostip))
        raise LookupError("Could not find appname {}".format(appname))

    def remove_env_keys(self, node, public_key):
        """
        Remove unwanted public keys from authorized keys under /root/.ssh/authorized_keys.
        """
        auth_keys = node.core.file_read('/root/.ssh/authorized_keys')
        data = []
        for key in auth_keys.splitlines():
            if key.startswith(public_key.strip()):
                continue
            data.append(key)
        data.append('') # end with empty line
        node.core.file_write('/root/.ssh/authorized_keys', '\n'.join(data))

    def add_env_keys(self, node, pub_key, priv_key):
        """
        adds pub and priv key to relative envs
        """
        # create dir
        ovcdir = '/var/ovc/'
        node.core.dir_ensure('{}/.ssh'.format(ovcdir), mode='600')
        node.core.dir_ensure('{}/root/.ssh'.format(ovcdir), mode='600')
        # add keys
        node.executor.sshclient.ssh_authorize('root', pub_key)
        node.core.file_write('{}/.ssh/id_rsa'.format(ovcdir), priv_key, mode='600')
        node.core.file_write('{}/.ssh/id_rsa.pub'.format(ovcdir), pub_key)
        node.core.file_write('{}/.ssh/authorized_keys'.format(ovcdir), pub_key)
        node.core.dir_ensure('/var/ovc/pxeboot/images/', mode='777')
        node.core.file_write('/var/ovc/pxeboot/images/pubkey', pub_key, mode='777')

    def get_new_keys(self):
        privatekey = self.first_node.core.file_read('/var/ovc/.ssh/id_rsa')
        pubkey = self.first_node.core.file_read('/var/ovc/.ssh/id_rsa.pub')
        return privatekey, pubkey

    def configure_keys(self):
        log('Configuring new keys')
        if not len(self.nodes):
            raise RuntimeError('cluster needs to be depployed before configuring keys')

        #get old keys
        old_pub_key = self.config['ssh']['public-key']
        # generate keys
        public_key_path = self.first_node.system.ssh.keygen(name='id_rsa').strip()
        private_key_path = public_key_path[:-4]

        # add keys to kuberntes
        public_key = self.first_node.core.file_read(public_key_path)
        private_key = self.first_node.core.file_read(private_key_path)

        # add keys to controller nodes
        for node in self.nodes:
            self.add_env_keys(node, public_key, private_key)
            self.remove_env_keys(node, old_pub_key)


class Node(AbstractConfig):
    def __init__(self, config, name):
        super().__init__(config)
        self.name = name
        self._node = None
        self.managementip = get_ipaddress(self.node['management']['ipaddress'])
        self._prefab = None
        self.config = config
        self.versions = config['environment']['versions']
        self.password = config['environment']['password']
        self.gid = config['environment']['grid']['id']
        self.managementips = list(get_controller_management_ips(config))
        self.fqdn = '{}.{}'.format(config['environment']['subdomain'], config['environment']['basedomain'])
        self.iyourl = 'https://itsyou.online/'

    def log(self, msg):
        if ISINTERACTIVE and self.name in status:
            status[self.name] = msg
            print_status(True)
        else:
            msg = "{}: {}".format(self.name, msg)
            log(msg)

    @property
    def prefab(self):
        if self._prefab is None:
            self._prefab = j.tools.prefab.getFromSSH(self.managementip)
        return self._prefab

    @property
    def node(self):
        if self._node is None:
            for node in itertools.chain(self.config['nodes']['cpu'], self.config['nodes']['storage']):
                if node['name'] == self.name:
                    self._node = node
                    break
            else:
                for node in self.config['controller']['hosts']:
                    if node['hostname'] == self.name:
                        self._node = node
                        break
                else:
                    raise LookupError("Failed to find node with name {}".format(self.name))
        return self._node


class JumpScale7(Node):
    def install_core(self):
        jsversion = self.versions['jumpscale']
        ovcversion = self.versions['openvcloud']
        env = {'AYSBRANCH': jsversion, 'JSBRANCH': jsversion}
        cmd = 'cd /tmp;rm -f install.sh;curl -k https://raw.githubusercontent.com/jumpscale7/jumpscale_core7/{}/install/install.sh > install.sh;bash install.sh'.format(jsversion)
        self.prefab.system.ssh.define_host('git.aydo.com')
        self.prefab.system.ssh.define_host('github.com')

        if self.prefab.bash.cmdGetPath('python', False) is False or self.prefab.bash.cmdGetPath('curl', False) is False:
            self.prefab.system.package.mdupdate()
            self.prefab.system.package.install('python')
            self.prefab.system.package.install('curl')
        if self.prefab.bash.cmdGetPath('js', False) is False:
            self.prefab.core.run(cmd, env=env)
        self.prefab.core.file_write('/opt/jumpscale7/hrd/system/atyourservice.hrd', AYSCONFIG.format(version=jsversion, ovcversion=ovcversion))
        self.prefab.core.file_write('/opt/jumpscale7/hrd/system/whoami.hrd', WHOAMI)

    def start(self):
        self.log("Start JumpScale services")
        self.prefab.core.run('ays start')

    def stop(self):
        self.log("Stop JumpScale services")
        self.prefab.core.run('ays stop')
        self.prefab.core.run('fuser -k 4446/tcp || true')

    def update_repo(self, account, repo, version):
        cmd = "jscode update -a '%s' -n '%s' -d -b %s --https" % (account, repo, version)
        self.prefab.core.run(cmd)


    def update(self):
        js_version = self.config['environment']['versions']['jumpscale']
        ovc_version = self.config['environment']['versions']['openvcloud']
        self.log("Updating JumpScale repos")
        self.update_repo('jumpscale7', '*', js_version)
        self.log("Updating OpenvCloud repos")
        self.update_repo('0-complexity', 'openvcloud', ovc_version)
        self.update_repo('0-complexity', 'openvcloud_installer', ovc_version)

    def restart(self):
        self.stop()
        self.start()

    def install_agent(self, roles=None):
        roles = roles or ['node']
        redisdata = {
            'instance.param.disk': '0',
            'instance.param.mem': '100',
            'instance.param.passwd': '',
            'instance.param.port': '9999',
            'instance.param.ip' : '0.0.0.0',
            'instance.param.unixsocket': '0'
        }
        self.ays_install('redis', instance='system', data=redisdata)
        masterips = ','.join(self.managementips)

        osisclientdata = {
            'param.osis.client.addr': masterips,
            'param.osis.client.login': 'root',
            'param.osis.client.passwd': self.password,
            'param.osis.client.port': '5544',
        }
        self.ays_install('osis_client', instance='main', data=osisclientdata)
        self.ays_install('osis_client', instance='jsagent', data=osisclientdata)

        agentcontrollerdata = {
            'agentcontroller.client.addr': masterips,
            'agentcontroller.client.login': 'node',
            'agentcontroller.client.passwd': '',
            'agentcontroller.client.port': '4444'
        }
        self.ays_install('agentcontroller_client', instance='main', data=agentcontrollerdata)

        agentdata = {
                'agentcontroller.connection': 'main',
                'grid.id': str(self.gid),
                'grid.node.roles': ','.join(roles),
                'osis.connection': 'jsagent',
        }
        self.ays_install('jsagent', instance='main', data=agentdata)


    def ays_install(self, package, domain='jumpscale', instance='main', data=None):
        self.log("Installing JumpScale package {}:{}".format(domain, package))
        datastr = ''
        data = data or {}
        for key, value in data.items():
            datastr += "{}:{} ".format(key, value)
        cmd = 'ays install -d {} -n {} -i {} --data "{}"'.format(domain, package, instance, datastr)
        self.prefab.core.run(cmd)

    def install_controller(self):
        self.install_core()
        self.install_agent(['node', 'controllernode'])

    def install_compute_node(self):
        self.install_core()
        self.install_agent()
        netinfo = {
            'vxbackend_vlan': self.config['network']['vxbackend']['vlan'],
            'vxbackend_ip': self.node['vxbackend']['ipaddress'],
            'gwmgmt_vlan': self.config['network']['gateway-management']['vlan'],
            'gwmgmt_ip': self.node['gateway-management']['ipaddress'],

        }
        data_net = {
            'netconfig.public_backplane.interfacename': 'backplane1',
            'netconfig.gw_mgmt_backplane.interfacename': 'backplane1',
            'netconfig.vxbackend.interfacename': 'backplane1',
            'netconfig.gw_mgmt.vlanid': netinfo['gwmgmt_vlan'],
            'netconfig.vxbackend.vlanid': netinfo['vxbackend_vlan'],
            'netconfig.gw_mgmt.ipaddr': netinfo['gwmgmt_ip'],
            'netconfig.vxbackend.ipaddr': netinfo['vxbackend_ip'],
        }

        data_cpu = {
            'instance.param.rootpasswd': self.password,
            'instance.param.master.addr': '',
            'instance.param.network.gw_mgmt_ip': netinfo['gwmgmt_ip'],
            'instance.param.grid.id': self.gid,
        }

        self.install_portal_client()
        self.ays_install('scaleout_networkconfig', domain='openvcloud', instance='main', data=data_net)
        self.ays_install('cb_cpunode_aio', domain='openvcloud', instance='main', data=data_cpu)


    def install_portal_client(self):
        portal_client = {
            'param.addr' : self.fqdn,
            'param.port': '443',
            'param.secret': self.password,
        }

        self.ays_install('portal_client', domain='jumpscale', instance='main', data=portal_client)

    def configure_iyo_api_key(self, apikey):
        # TODO: check if we can do this in js9
        label = apikey['label']
        clientid = self.config['itsyouonline']['clientId']
        secret = self.config['itsyouonline']['clientSecret']
        accesstokenparams = {'grant_type': 'client_credentials', 'client_id': clientid, 'client_secret': secret}
        accesstoken = requests.post(os.path.join(self.iyourl, 'v1', 'oauth', 'access_token'), params=accesstokenparams)
        token = accesstoken.json()['access_token']
        authheaders = {'Authorization': 'token %s' % token}
        result = requests.get(os.path.join(self.iyourl, 'api', 'organizations', clientid,
                                           'apikeys', label), headers=authheaders)
        if result.status_code == 200:
            stored_apikey = result.json()
            if apikey['callbackURL'] != stored_apikey['callbackURL']:
                requests.delete(os.path.join(self.iyourl, 'api', 'organizations', clientid,
                                             'apikeys', label), headers=authheaders)
                apikey = {}
            else:
                apikey = stored_apikey

        if 'secret' not in apikey:
            result = requests.post(os.path.join(self.iyourl, 'api', 'organizations',
                                                clientid, 'apikeys'), json=apikey, headers=authheaders)
            apikey = result.json()
        return apikey

    def install_storage_node(self):
        self.install_core()
        self.install_agent()
        self.install_portal_client()

        data_storage = {
            'param.rootpasswd': self.password,
            'param.master.addr': '',
            'param.grid.id': self.gid,
        }

        self.ays_install('cb_storagenode_aio', domain='openvcloud', instance='main', data=data_storage)
        self.ays_install('cb_storagedriver_aio', domain='openvcloud', instance='main', data=data_storage)
        status, output, _ = self.prefab.core.run('ovs config get "ovs/framework/hosts/$(cat /etc/openvstorage_id)/type"')
        if 'MASTER' in output:
            self.prefab.core.run("python /opt/code/github/0-complexity/openvcloud/scripts/ovs/alba-create-user.py")
            ovscallbackurl = 'https://ovs-{}/api/oauth2/redirect/'.format(self.fqdn)
            apikey = {
                'label': 'ovs-{}'.format(self.config['environment']['subdomain']),
                'clientCredentialsGrantType': False,
                'callbackURL' : ovscallbackurl
            }
            apikey = self.configure_iyo_api_key(apikey)

            oauth_token_uri = os.path.join(self.iyourl, 'v1/oauth/access_token')
            oauth_authorize_uri = os.path.join(self.iyourl, 'v1/oauth/authorize')

            data_oauth = {'instance.oauth.id': self.config['itsyouonline']['clientId'],
                        'instance.oauth.secret': apikey['secret'],
                        'instance.oauth.authorize_uri': oauth_authorize_uri,
                        'instance.oauth.token_uri': oauth_token_uri}
            self.ays_install('openvstorage_oauth', 'openvcloud', instance='main', data=data_oauth)



class Cluster(AbstractConfig):
    """
    Cluster abstraction layer to allow for easier manipulation.
    """
    def __init__(self, config_path, development=False):
        super().__init__(config_path)
        self.k8s_config = '/root/.kube/{}.conf'.format(self.config['environment']['subdomain'])
        self.join_line = ''
        self.scripts_dir = '{}/kuberesources/'.format(REPO_PATH)
        self.development = development

    def install_kubernetes_cluster(self):
        """
        Will install a kubernetes master and minion nodes on the first and rest of the node list respectively.
        """
        log('Installing k8 cluster')
        for node in self.nodes:
            node.core.run("sudo sed -i.bak '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab")

        managementips = list(get_controller_management_ips(self.config))
        k8s_config_data, self.join_line = self.prefab.virtualization.kubernetes.multihost_install(
                self.nodes, unsafe=True, reset=True, external_ips=managementips)
        self.prefab.core.dir_ensure('/root/.kube')
        self.prefab.core.file_write(self.k8s_config, k8s_config_data)
        if not self.prefab.core.file_exists('/root/.kube/config'):
            self.prefab.core.file_write('/root/.kube/config', k8s_config_data)

    def clone_repo(self, url=REPO_URL):
        for node in self.nodes:
            node.tools.git.pullRepo(url)

    def _install_kubectl(self, prefab):
        if not prefab.core.file_exists('/usr/local/bin/kubectl'):
            prefab.virtualization.kubernetes.install_kube_client(location='/usr/local/bin/kubectl')


    def install_controller(self):
        """
        Will use existing yaml or config scripts in this dir as well as jumpscale modules to install the controller setup on the
        cluster. Creating the relevant deployments, services, and mounts
        """
        log('Install kubectl')
        for node in self.nodes:
            self._install_kubectl(node)
        self._install_kubectl(self.prefab)
        self.clone_repo()

        log('Preparing directories')
        directories = self.config['controller'].get('directories')
        for node in self.nodes:
            for directory in directories:
                node.core.dir_ensure(directory['path'], mode='777')

        log('Writing certificates')
        copiedpaths = {}
        ssldir = '/var/ovc/ssl/'
        for key, value in self.config['environment']['ssl'].items():
            path = os.path.join(value['path'], '')
            if path not in copiedpaths:
                for node in self.nodes:
                    node.core.upload(path, ssldir)
                copiedpaths[path] = ssldir

        self.install_teleport()
        self.configure_keys()
        self.kube_client_apply()

    def apply_template(self, template, kind):
        self.kubectl('apply -f {template}'.format(template=template))
        template_name = j.sal.fs.getBaseName(template)
        timeout = time.time() + 240
        print('Waiting for {} to be ready..'.format(template_name))
        while time.time() < timeout:
            template_basename = template.split('/')[-1]
            template_name = template_basename.strip('.yaml') if template_basename.endswith('.yaml') else template_basename
            _, out, _ = self.kubectl('get %s %s -o json' % (kind, template_name))
            json_out = j.data.serializer.json.loads(out)
            ready = False
            if kind == 'deployment':
                for condition in  json_out['status'].get('conditions'):
                    if condition['type'] == 'Available' and condition['status'] == 'True':
                        ready = True
                        break
            if kind == 'statefulset':
                if json_out['status']['replicas'] == json_out['status'].get('readyReplicas'):
                    ready = True
                    break
            if ready:
                break
        else:
            raise j.exceptions.Timeout('Deploying {} took longer than expected. Exiting..'.format(template_name))

    def dump(self, location, data, dumper=j.data.serializer.yaml.dumps):
        data = dumper(data)
        self.first_node.core.file_write(location, data)

    def load(self, location, loader=j.data.serializer.yaml.loads):
        data = self.first_node.core.file_read(location)
        return loader(data)

    def write_service_template(self, template, externalips):
        template_loc = '{dir}{name}/{name}-service.yaml'.format(dir=self.scripts_dir, name=template)
        data = self.load(template_loc)
        if data['spec'].get('type') == 'NodePort':
            data['spec']['externalIPs'] = externalips
            self.dump(template_loc, data)
        return template_loc

    def _apply_nginx(self, name):
        pubips = list(get_controller_public_ips(self.config))
        loc = self.write_service_template(name, pubips)
        self.kubectl('apply -f {}'.format(loc))

    def upgrade(self):
        self._apply_nginx('upgrader')

    def revert(self):
        """
        Reconfigure nginx service to working state
        """
        self._apply_nginx('nginx')

    def prepare_templates(self):
        log('Preparing resources')
        def write_deployment_template(template):
            template_loc = '{dir}{name}/{name}-deployment.yaml'.format(dir=self.scripts_dir, name=template)
            data = self.load(template_loc)
            for container in data['spec']['template']['spec']['containers']:
                volmounts = container.setdefault('volumeMounts', [])
                volmounts.append({'name': 'code', 'mountPath': '/opt/code/github/0-complexity', 'subPath': 'github/0-complexity'})
                volmounts.append({'name': 'code', 'mountPath': '/opt/code/github/jumpscale7', 'subPath': 'github/jumpscale7'})
            volumes = data['spec']['template']['spec'].setdefault('volumes', [])
            volumes.append({'name': 'code', 'hostPath': {'path': '/opt/code', 'type': 'Directory'}})
            self.dump(template_loc, data)

        self.first_node.core.upload('{}/scripts/kubernetes/'.format(REPO_PATH), self.scripts_dir)
        templates = ['agentcontroller', 'osis']
        externalips = list(get_controller_management_ips(self.config))
        for template in templates:
            self.write_service_template(template, externalips)

        pubtemplates = ['nginx', 'zero-access', 'management']
        pubips = list(get_controller_public_ips(self.config))
        for template in pubtemplates:
            self.write_service_template(template, pubips)

        if self.development:
            for template in ['osis', 'agentcontroller', 'portal']:
                write_deployment_template(template)
            for repo in DEV_REPOS:
                self.clone_repo(repo)

        stat_template = self.scripts_dir + '/stats-collector/stats-deployment.yaml'
        data = self.load(stat_template)
        data['spec']['template']['spec']['containers'][0]['args'][4] = self.config['network']['management']['network']
        self.dump(stat_template, data)

    def write_system_config(self):
        log('Write system config')
        tmpdir = self.first_node.core.file_get_tmp_path()
        self.first_node.core.dir_ensure(tmpdir)
        tmpfile = j.sal.fs.joinPaths(tmpdir, 'system-config.yaml')
        try:
            self.dump(tmpfile, self.config)
            self.kubectl('create configmap system-config --from-file {path}'.format(
                            path=tmpfile))
        finally:
            self.first_node.core.dir_remove(tmpdir)

    def install_teleport(self):
        for node in self.nodes:
            log('Installing teleport {}'.format(node.core.hostname))
            ssl_name = self.config['environment']['ssl']['root']['name']
            node.apps.teleport.package_install(extra_paths=['/usr/local/bin'])
            node.apps.teleport.write_config(name=node.core.hostname, key_path='/var/ovc/ssl/%s.key' % ssl_name,
                                            cert_path='/var/ovc/ssl/%s.crt' % ssl_name)
            node.apps.teleport.start()
            github = self.config['support']['github']
            teams = ['/'.join([team['org_name'], team['team_name']]) for team in github['teams']]
            node.apps.teleport.apply_permissions(
                name=node.core.hostname,
                client_id=github['client_id'],
                client_secret=github['client_secret'],
                teams=teams,
                exposed_ip='{}.{}'.format(self.config['environment']['subdomain'], self.config['environment']['basedomain'])
            )

    def kube_client_apply(self):
        self.prepare_templates()
        self.write_system_config()

        self.kubectl('apply -f {path}/rbac.yaml'.format(path=self.scripts_dir))
        deployments = ['influxdb', 'osis', 'agentcontroller', 'stats-collector',
                       'portal', 'pxeboot', 'zero-access', 'management', 'controller-jsagent', 'nginx']
        statefulSets = ['mongo', 'syncthing']
        for statefulSet in statefulSets:
            template_file = self.scripts_dir + statefulSet
            self.apply_template(template_file, kind='statefulset')
        for deployment in deployments:
            template_file = self.scripts_dir + deployment
            self.apply_template(template_file, kind='deployment')

        self.grafana_apply('{}/grafana'.format(self.scripts_dir))

    def grafana_apply(self, grafana_dir):
        log('Installing grafana')
        datasourcepath = '{}/provisioning/datasources/influx.yaml'.format(grafana_dir)
        dashboardpath = '{}/sources/templates'.format(grafana_dir)
        datasource = self.load(datasourcepath)
        datasourcename = 'controller_{}'.format(self.config['environment']['subdomain'])
        datasource['datasources'][0]['name'] = datasourcename
        self.dump(datasourcepath, datasource)
        for dashboardfile in self.first_node.core.find(dashboardpath, pattern='*.json'):
            db = self.load(dashboardfile, j.data.serializer.json.loads)
            db['id'] = None
            db['title'] += " ({})".format(self.config['environment']['subdomain'])
            for row in db['rows']:
                for panel in row['panels']:
                    panel['datasource'] = datasourcename
            if 'templating' in db:
                for item in db['templating']['list']:
                    item['datasource'] = datasourcename
            self.dump(dashboardfile, db, j.data.serializer.json.dumps)
        cmd = """
        cd {dir}
        kubectl create configmap grafana-provisioning-datasources --from-file=provisioning/datasources
        kubectl create configmap grafana-provisioning-dashboards --from-file=provisioning/dashboards
        kubectl create configmap grafana-dashboards --from-file=sources/templates
        kubectl apply -f grafana-service.yaml
        kubectl apply -f grafana-deployment.yaml
        """.format(config=self.k8s_config, dir=grafana_dir)
        self.first_node.core.execute_bash(cmd)

    def delete(self):
        log('Deleting configmaps')
        self.first_node.core.execute_bash('kubectl delete configmap --all --namespace default')
        log('Deleting services')
        self.first_node.core.execute_bash("kubectl delete service --namespace default -l 'name != nginx'")
        log('Deleting statefulset')
        self.first_node.core.execute_bash('kubectl delete statefulset --all --namespace default')
        log('Deleting deployments')
        self.first_node.core.execute_bash('kubectl delete deploy --all --namespace default')

    def update(self):
        log('Restarting cluster...')
        self.delete()
        self.kube_client_apply()

class PNode(JumpScale7):
    def ipmi(self, *args):
        ipmiip = get_ipaddress(self.node['ipmi']['ipaddress'])
        cmd = "ipmitool -I lanplus -H {} -U {} -P {} ".format(ipmiip, self.node['ipmi']['username'], self.node['ipmi']['password'])
        cmd += " ".join(args)
        self.first_node.core.run(cmd)

    def reboot(self):
        self.log('Rebooting')
        self.ipmi('chassis power cycle')

    def is_up(self):
        return j.sal.nettools.tcpPortConnectionTest(self.managementip, 22)

    def wait_up(self, timeout=600):
        j.sal.nettools.waitConnectionTest(self.managementip, 22, timeout)
        if not self.is_up():
            raise RuntimeError('Node did not come up in {}seconds'.format(timeout))

    def enable_pxe(self):
        self.log('Enable PXE')
        prefab = self.get_node_from_appname('pxeboot')
        self.ipmi('chassis bootdev pxe')
        macaddress = str(netaddr.EUI(self.node['management']['macaddress'])).lower()
        dst = '/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}'.format(macaddress)
        src = '../../conf/tftp-911boot'
        prefab.core.file_link(src, dst)

    def disable_pxe(self):
        self.log('Disable PXE')
        prefab = self.get_node_from_appname('pxeboot')
        macaddress = str(netaddr.EUI(self.node['management']['macaddress'])).lower()
        dst = '/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}'.format(macaddress)
        prefab.core.file_unlink(dst)

    def install_os(self):
        self.enable_pxe()
        self.reboot()
        time.sleep(20)
        self.log('Waiting for 911 boot')
        self.wait_up()
        self.log('Installing OS')
        status, output, err = self.prefab.core.run('cd /root/tools/; ./Install')
        if status != 0:
            raise RuntimeError('Failed to install OS')
        self.disable_pxe()
        self.reboot()
        time.sleep(20)
        self.log('Waiting for OS boot')
        self.wait_up()

###                    ###
# Command line interface #
###                    ###

@click.group()
@click.option('--config', help='Config file to deploy the cluster', envvar='ENV_CONFIG')
@click.option('--loglevel', default=50, help='Set loglevel', type=int)
@click.pass_context
def cli(ctx, config, loglevel):
    j.logger.set_level(loglevel)
    if not config:
        raise j.exceptions.Input('Please specify a config file')
    config = prepare_config(config)
    ctx.obj = config

@cli.group()
def cluster():
    pass

@cluster.group()
def resources():
    pass

@cluster.group()
def resource():
    pass

@cli.group()
def node():
    pass

@cli.group()
def cpu():
    pass

@cli.group()
def controller():
    pass

@cli.group()
def storage():
    pass

@cli.group()
def image():
    pass


@image.command('deploy')
@click.option('--name', help='Image name')
@click.pass_context
def image_deploy(ctx, name):
    storage = random.choice(ctx.obj['nodes']['storage'])
    jumpscale = JumpScale7(ctx.obj, storage['name'])
    jumpscale.ays_install(name, 'openvcloud')


@cluster.command('install_teleport')
@click.pass_context
def install_teleport(ctx):
    cluster = Cluster(ctx.obj)
    cluster.install_teleport()

@node.command('action')
@click.option('--name', help='Node name pass all to apply on all comma seperate list for multiple')
@click.argument('action')
@click.pass_context
def node_action(ctx, name, action):
    """
    Execute command for node
    """
    if name != 'all' and ',' not in name:
        cluster = PNode(ctx.obj, name)
        getattr(cluster, action)()
    else:
        if name == 'all':
            pnames = [pnode['name'] for pnode in itertools.chain(ctx.obj['nodes']['cpu'], ctx.obj['nodes']['storage'])]
        else:
            pnames = name.split(',')
        pnode_action(pnames, action, ctx.obj)

@node.command('update')
@click.pass_context
def node_update(ctx):
    return _node_update(ctx)

def _node_update(ctx):
    """
    Update all pnodes
    """
    config = ctx.obj
    pnames = get_pnodes_names(config)
    pnode_action(pnames, 'update', config)
    pnode_action(pnames, 'restart', config)

@resources.command('writeconfig')
@click.pass_context
def write_system_config(ctx):
    """
    Write or update system-config in kubernetes configmap based on yaml file
    """
    cluster = Cluster(ctx.obj)
    cluster.write_system_config()


@resources.command('write')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def write_kube_resources(ctx, development):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj, development)
    cluster.prepare_templates()


@resources.command('applyall')
@click.pass_context
def kube_apply_all(ctx):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj)
    cluster.kube_client_apply()


@resources.command('apply')
@click.option('--path', help='path to template')
@click.pass_context
def kube_apply(ctx, path):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj)
    cluster.apply_template(path, kind='deployment')

@cluster.command('deploy')
@click.option('--configure-cluster/--no-configure-cluster', help='Configure kubernetes cluster', default=True)
@click.pass_context
def deploy_cluster(ctx, configure_cluster):
    """
    Deploy will create the cluster machines and deploy kubernetes cluster on top of them.
    """
    cluster = Cluster(ctx.obj)
    if configure_cluster:
        cluster.install_kubernetes_cluster()
    cluster.install_controller()

@cluster.command('update')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def update_cluster(ctx, development):
    return _update_cluster(ctx, development)

def _update_cluster(ctx, development):
    """
    Will update the cluster.
    """
    cluster = Cluster(ctx.obj, development)
    cluster.update()

@cluster.command('upgrade')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def upgrade_cluster(ctx, development):
    """
    Will upgrade the env.
    """
    if not development:
        j.logger.set_level(30)
    config = ctx.obj
    cluster = Cluster(config, development)
    cluster.upgrade()
    try:
        pnames = get_pnodes_names(config)
        pnode_action(pnames, 'stop', config)
        pnode_action(pnames, 'update', config)
        cluster.update()
        pnode_action(pnames, 'start', config)
        print('Updating env done.')
    except:
        cluster.revert()
        print("Failed to update env.")

@cpu.command('deploy')
@click.option('--name', help='Node name to configure')
@click.pass_context
def deploy_cpu(ctx, name):
    jumpscale = JumpScale7(ctx.obj, name)
    jumpscale.install_compute_node()


@controller.command('deploy')
@click.option('--name', help='Node name to configure')
@click.pass_context
def deploy_controller(ctx, name):
    jumpscale = JumpScale7(ctx.obj, name)
    jumpscale.install_controller()


@storage.command('deploy')
@click.option('--name', help='Node name to configure')
@click.pass_context
def deploy_storage(ctx, name):
    jumpscale = JumpScale7(ctx.obj, name)
    jumpscale.install_storage_node()





if __name__ == '__main__':
    cli()


