#!/usr/bin/env python3
from js9 import j
import netaddr
import click
import itertools
import os
import requests
import random
import sys
import time
import jsonschema
import concurrent.futures.process

REPO_URL = 'https://github.com/0-complexity/openvcloud_installer'
DEV_REPOS = ['https://github.com/0-complexity/openvcloud', 'https://github.com/jumpscale7/jumpscale_core7', 'https://github.com/jumpscale7/jumpscale_portal']
REPO_PATH = '/opt/code/github/0-complexity/openvcloud_installer'
AYSCONFIG = '''
metadata.jumpscale             =
    branch:'{version}',
    url:'git@github.com:jumpscale7/ays_jumpscale7',

metadata.openvcloud            =
    branch:'{ovcversion}',
    url:'git@github.com:0-complexity/openvcloud_ays',
'''

WHOAMI = '''
email                          =

fullname                       =

git.login                      = 'ssh'
git.passwd                     = 'ssh'
'''
ERROR_COLOR = u'\u001b[31m'
RESET_COLOR  = u'\u001b[0m'

def get_ipaddress(net):
    return str(netaddr.IPNetwork(net).ip)

def get_pnodes_names(config):
    pnodes_names = []
    for host in config['controller']['hosts']:
        pnodes_names.append(host['hostname'])
    for pnode in itertools.chain(config['nodes']['cpu'], config['nodes']['storage']):
        pnodes_names.append(pnode['name'])
    return pnodes_names

def pnode_action(pnames, action, config):
    futures = {}
    running_futures = {}
    results = ''
    with concurrent.futures.process.ProcessPoolExecutor(len(pnames)) as proc:
        for pname in pnames:
            print("{action} node: {name} started".format(action=action, name=pname))
            jumpscale = JumpScale7(config, pname)
            func = getattr(jumpscale, action)
            fut = proc.submit(func)
            futures[pname] =  fut
            running_futures[pname] = fut

        def print_status():
            messages = ''
            future_error = False
            for name, future in futures.items():
                if future.done() and name in running_futures:
                    if future.exception():
                        messages += '{color}Error when performing node {action} {name}: {reset}{exc} \n'.format(color=ERROR_COLOR, action=action, name=name, reset=RESET_COLOR, exc=str(future.exception()))
                        running_futures.clear()
                        future_error = True
                    else:
                        messages += '{action} done on node: {name} \n'.format(action=action, name=name)
                        running_futures.pop(name)
            return future_error, messages

        print("Waiting for {action} node to be done...".format(action=action))
        while running_futures:
            error, msg = print_status()
            results += msg
            time.sleep(1)
        print(results)
        if error:
            raise RuntimeError('Error while performaing action {}'.format(action))

def prepare_config(config_path):
    def _helper(nodes):
        for node in nodes:
            net = netaddr.IPNetwork(value['network'])
            ip = net.ip + node['ip-lsb']
            if key not in node:
                node[key] = {}
            node[key]['ipaddress'] = '{ip}/{sub}'.format(ip=ip, sub=net.prefixlen)

    config = j.data.serializer.yaml.load(config_path)
    validator = j.data.serializer.json.load('{}/scripts/kubernetes/config/config-validator.json'.format(REPO_PATH))
    try:
        jsonschema.validate(config, validator)
    except Exception as error:
        message = getattr(error, "message", str(type(error)))
        tree = ''
        for seq in getattr(error, "path", list()):
            if isinstance(seq, int):
                tree += '/<sequence {}>'.format(seq)
            else:
                tree += "/{}".format(seq)

        validator = getattr(error, "validator")
        if  validator == 'type':
            message = '{msg} at {tree}'.format(msg=message, tree=tree)
        elif validator == 'required':
            message = "{msg} in config at {tree}. Please check example config for reference.".format(msg=message, tree=tree)
        raise j.exceptions.RuntimeError(message)

    for key, value in config['network'].items():
        if 'network' in value:
            for nodes in config['nodes'].values():
                _helper(nodes)
            _helper(config['controller']['hosts'])
    cmd = 'echo \'{}\' | ssh-keygen -y -f /dev/stdin'.format(config['ssh']['private-key'])
    _, public_key, _ = j.sal.process.execute(cmd, showout=False)
    config['ssh']['public-key'] = public_key
    j.clients.ssh.start_ssh_agent()
    if j.sal.fs.exists('~/.ssh/id_rsa'):
        j.clients.ssh.load_ssh_key('~/.ssh/id_rsa')
    key = config['ssh']['private-key']
    j.sal.process.execute('echo "%s" | ssh-add /dev/stdin' % key, showout=False)
    return config

def get_controller_management_ips(config):
    for host in config['controller']['hosts']:
        yield get_ipaddress(host['management']['ipaddress'])

def get_controller_public_ips(config):
    for host in config['controller']['hosts']:
        yield get_ipaddress(host['fallback']['ipaddress'])

class AbstractConfig:
    def __init__(self, config):
        self.config = config
        self._prefab = None
        self._nodes = None

    @property
    def prefab(self):
        if self._prefab is None:
            self._prefab = j.tools.prefab.local
        return self._prefab

    @property
    def nodes(self):
        if self._nodes is None:
            nodes = []
            for address in get_controller_public_ips(self.config):
                executor = j.tools.executor.getSSHBased(address, usecache=False)
                nodes.append(j.tools.prefab.get(executor))
            self._nodes = nodes
        return self._nodes

    @property
    def first_node(self):
        return self.nodes[0]

    def kubectl(self, *args, **kwargs):
        cmd = "kubectl "
        cmd += " ".join(args)
        return self.first_node.core.run(cmd, **kwargs)

    def get_node_from_appname(self, appname):
        status, output, _ = self.kubectl("get pods -o json")
        data = j.data.serializer.json.loads(output)
        for pod in data['items']:
            if pod['metadata']['labels'].get('app') != appname:
                continue
            hostip = pod['status']['hostIP']
            for node in self.nodes:
                netinfo = node.system.net.getInfo()
                for nic in netinfo:
                    for ip in nic['ip']:
                        if ip == hostip:
                            return node

            raise LookupError("Could not find host with IP {}".format(hostip))
        raise LookupError("Could not find appname {}".appname)

    def remove_env_keys(self, node, public_key):
        """
        Remove unwanted public keys from authorized keys under /root/.ssh/authorized_keys.
        """
        auth_keys = node.core.file_read('/root/.ssh/authorized_keys')
        data = []
        for key in auth_keys.splitlines():
            if key.startswith(public_key.strip()):
                continue
            data.append(key)
        data.append('') # end with empty line
        node.core.file_write('/root/.ssh/authorized_keys', '\n'.join(data))

    def add_env_keys(self, node, pub_key, priv_key, ovcdir='/var/ovc/'):
        """
        adds pub and priv key to relative envs
        """
        # create dir
        node.core.dir_ensure('{}/.ssh'.format(ovcdir), mode='600')
        node.core.dir_ensure('{}/root/.ssh'.format(ovcdir), mode='600')
        # add keys
        node.executor.sshclient.ssh_authorize('root', pub_key)
        node.core.file_write('{}/.ssh/id_rsa'.format(ovcdir), priv_key, mode='600')
        node.core.file_write('{}/.ssh/id_rsa.pub'.format(ovcdir), pub_key)
        node.core.file_write('{}/.ssh/authorized_keys'.format(ovcdir), pub_key)

    def get_new_keys(self):
        privatekey = self.first_node.core.file_read('/var/ovc/.ssh/id_rsa')
        pubkey = self.first_node.core.file_read('/var/ovc/.ssh/id_rsa.pub')
        return privatekey, pubkey

    def configure_keys(self):
        if not len(self.nodes):
            raise RuntimeError('cluster needs to be depployed before configuring keys')

        #get old keys
        old_pub_key = self.config['ssh']['public-key']
        # generate keys
        public_key_path = self.first_node.system.ssh.keygen(name='id_rsa').strip()
        private_key_path = public_key_path[:-4]

        # add keys to kuberntes
        public_key = self.first_node.core.file_read(public_key_path)
        private_key = self.first_node.core.file_read(private_key_path)

        # add keys to controller nodes
        for node in self.nodes:
            self.add_env_keys(node, public_key, private_key)
            self.remove_env_keys(node, old_pub_key)


class Node(AbstractConfig):
    def __init__(self, config, name):
        super().__init__(config)
        self.name = name
        self._node = None
        self.managementip = get_ipaddress(self.node['management']['ipaddress'])
        self._prefab = None
        self.config = config
        self.versions = config['environment']['versions']
        self.password = config['environment']['password']
        self.gid = config['environment']['grid']['id']
        self.managementips = list(get_controller_management_ips(config))
        self.fqdn = '{}.{}'.format(config['environment']['subdomain'], config['environment']['basedomain'])
        self.iyourl = 'https://itsyou.online/'

    @property
    def prefab(self):
        if self._prefab is None:
            self._prefab = j.tools.prefab.getFromSSH(self.managementip)
        return self._prefab

    def update_keys(self):
        self.remove_env_keys(self.prefab, self.config['ssh']['public-key'])
        privatekey, publickey = self.get_new_keys()
        self.add_env_keys(self.prefab, publickey, privatekey, '/root')

    @property
    def node(self):
        if self._node is None:
            for node in itertools.chain(self.config['nodes']['cpu'], self.config['nodes']['storage']):
                if node['name'] == self.name:
                    self._node = node
                    break
            else:
                for node in self.config['controller']['hosts']:
                    if node['hostname'] == self.name:
                        self._node = node
                        break
                else:
                    raise LookupError("Failed to find node with name {}".format(self.name))
        return self._node

class JumpScale7(Node):
    def install_core(self):
        self.update_keys()
        jsversion = self.versions['jumpscale']
        ovcversion = self.versions['openvcloud']
        env = {'AYSBRANCH': jsversion, 'JSBRANCH': jsversion}
        cmd = 'cd /tmp;rm -f install.sh;curl -k https://raw.githubusercontent.com/jumpscale7/jumpscale_core7/{}/install/install.sh > install.sh;bash install.sh'.format(jsversion)
        self.prefab.system.ssh.define_host('git.aydo.com')
        self.prefab.system.ssh.define_host('github.com')

        if self.prefab.bash.cmdGetPath('python', False) is False or self.prefab.bash.cmdGetPath('curl', False) is False:
            self.prefab.system.package.mdupdate()
            self.prefab.system.package.install('python')
            self.prefab.system.package.install('curl')
        if self.prefab.bash.cmdGetPath('js', False) is False:
            self.prefab.core.run(cmd, env=env)
        self.prefab.core.file_write('/opt/jumpscale7/hrd/system/atyourservice.hrd', AYSCONFIG.format(version=jsversion, ovcversion=ovcversion))
        self.prefab.core.file_write('/opt/jumpscale7/hrd/system/whoami.hrd', WHOAMI)

    def start(self):
        self.prefab.core.run('ays start')

    def stop(self):
        self.prefab.core.run('ays stop')
        self.prefab.core.run('fuser -k 4446/tcp || true')

    def update_repo(self, account, repo, version):
        cmd = "jscode update -a '%s' -n '%s' -d -b %s" % (account, repo, version)
        self.prefab.core.run(cmd)


    def update(self):
        js_version = self.config['environment']['versions']['jumpscale']
        ovc_version = self.config['environment']['versions']['openvcloud']
        self.update_repo('jumpscale7', '*', js_version)
        self.update_repo('0-complexity', 'openvcloud', ovc_version)
        self.update_repo('0-complexity', 'openvcloud_installer', ovc_version)
        self.update_repo('0-complexity', 'selfhealing', ovc_version)
        self.update_repo('0-complexity', 'g8vdc', ovc_version)
        self.update_repo('0-complexity', 'openvcloud_ays', ovc_version)

    def restart(self):
        self.stop()
        self.start()

    def install_agent(self, roles=None):
        roles = roles or ['node']
        redisdata = {
            'instance.param.disk': '0',
            'instance.param.mem': '100',
            'instance.param.passwd': '',
            'instance.param.port': '9999',
            'instance.param.ip' : '0.0.0.0',
            'instance.param.unixsocket': '0'
        }
        self.ays_install('redis', instance='system', data=redisdata)
        masterips = ','.join(self.managementips)

        osisclientdata = {
            'param.osis.client.addr': masterips,
            'param.osis.client.login': 'root',
            'param.osis.client.passwd': self.password,
            'param.osis.client.port': '5544',
        }
        self.ays_install('osis_client', instance='main', data=osisclientdata)
        self.ays_install('osis_client', instance='jsagent', data=osisclientdata)

        agentcontrollerdata = {
            'agentcontroller.client.addr': masterips,
            'agentcontroller.client.login': 'node',
            'agentcontroller.client.passwd': '',
            'agentcontroller.client.port': '4444'
        }
        self.ays_install('agentcontroller_client', instance='main', data=agentcontrollerdata)

        agentdata = {
                'agentcontroller.connection': 'main',
                'grid.id': str(self.gid),
                'grid.node.roles': roles,
                'osis.connection': 'jsagent',
        }
        self.ays_install('jsagent', instance='main', data=agentdata)


    def ays_install(self, package, domain='jumpscale', instance='main', data=None):
        datastr = ''
        data = data or {}
        for key, value in data.items():
            datastr += "{}:{} ".format(key, value)
        cmd = 'ays install -d {} -n {} -i {} --data "{}"'.format(domain, package, instance, datastr)
        self.prefab.core.run(cmd)

    def install_controller(self):
        self.install_core()
        self.install_agent(['node', 'controllernode'])

    def install_compute_node(self):
        self.install_core()
        self.install_agent()
        netinfo = {
            'vxbackend_vlan': self.config['network']['vxbackend']['vlan'],
            'vxbackend_ip': self.node['vxbackend']['ipaddress'],
            'gwmgmt_vlan': self.config['network']['gateway-management']['vlan'],
            'gwmgmt_ip': self.node['gateway-management']['ipaddress'],

        }
        data_net = {
            'netconfig.public_backplane.interfacename': 'backplane1',
            'netconfig.gw_mgmt_backplane.interfacename': 'backplane1',
            'netconfig.vxbackend.interfacename': 'backplane1',
            'netconfig.gw_mgmt.vlanid': netinfo['gwmgmt_vlan'],
            'netconfig.vxbackend.vlanid': netinfo['vxbackend_vlan'],
            'netconfig.gw_mgmt.ipaddr': netinfo['gwmgmt_ip'],
            'netconfig.vxbackend.ipaddr': netinfo['vxbackend_ip'],
        }

        data_cpu = {
            'instance.param.rootpasswd': self.password,
            'instance.param.master.addr': '',
            'instance.param.network.gw_mgmt_ip': netinfo['gwmgmt_ip'],
            'instance.param.grid.id': self.gid,
        }

        self.install_portal_client()

        print('installing network')
        self.ays_install('scaleout_networkconfig', domain='openvcloud', instance='main', data=data_net)

        print('installing cpu node')
        self.ays_install('cb_cpunode_aio', domain='openvcloud', instance='main', data=data_cpu)


    def install_portal_client(self):
        portal_client = {
            'param.addr' : self.fqdn,
            'param.port': '443',
            'param.secret': self.password,
        }

        print('Install portal client')
        self.ays_install('portal_client', domain='jumpscale', instance='main', data=portal_client)

    def configure_iyo_api_key(self, apikey):
        # TODO: check if we can do this in js9
        label = apikey['label']
        clientid = self.config['itsyouonline']['clientId']
        secret = self.config['itsyouonline']['clientSecret']
        accesstokenparams = {'grant_type': 'client_credentials', 'client_id': clientid, 'client_secret': secret}
        accesstoken = requests.post(os.path.join(self.iyourl, 'v1', 'oauth', 'access_token'), params=accesstokenparams)
        token = accesstoken.json()['access_token']
        authheaders = {'Authorization': 'token %s' % token}
        result = requests.get(os.path.join(self.iyourl, 'api', 'organizations', clientid,
                                           'apikeys', label), headers=authheaders)
        if result.status_code == 200:
            stored_apikey = result.json()
            if apikey['callbackURL'] != stored_apikey['callbackURL']:
                print('API key does not match callback url deleting it')
                requests.delete(os.path.join(self.iyourl, 'api', 'organizations', clientid,
                                             'apikeys', label), headers=authheaders)
                apikey = {}
            else:
                apikey = stored_apikey

        if 'secret' not in apikey:
            print('Creating API key')
            result = requests.post(os.path.join(self.iyourl, 'api', 'organizations',
                                                clientid, 'apikeys'), json=apikey, headers=authheaders)
            apikey = result.json()
        return apikey

    def install_storage_node(self):
        self.install_core()
        self.install_agent()
        self.install_portal_client()

        data_storage = {
            'param.rootpasswd': self.password,
            'param.master.addr': '',
            'param.grid.id': self.gid,
        }

        print('[+] installing storage node')
        self.ays_install('cb_storagenode_aio', domain='openvcloud', instance='main', data=data_storage)
        self.ays_install('cb_storagedriver_aio', domain='openvcloud', instance='main', data=data_storage)
        status, output, _ = self.prefab.core.run('ovs config get "ovs/framework/hosts/$(cat /etc/openvstorage_id)/type"')
        if 'MASTER' in output:
            self.prefab.core.run("python /opt/code/github/0-complexity/openvcloud/scripts/ovs/alba-create-user.py")
            ovscallbackurl = 'https://ovs-{}/api/oauth2/redirect/'.format(self.fqdn)
            apikey = {
                'label': 'ovs-{}'.format(self.config['environment']['subdomain']),
                'clientCredentialsGrantType': False,
                'callbackURL' : ovscallbackurl
            }
            apikey = self.configure_iyo_api_key(apikey)

            oauth_token_uri = os.path.join(self.iyourl, 'v1/oauth/access_token')
            oauth_authorize_uri = os.path.join(self.iyourl, 'v1/oauth/authorize')

            data_oauth = {'instance.oauth.id': self.config['itsyouonline']['clientId'],
                        'instance.oauth.secret': apikey['secret'],
                        'instance.oauth.authorize_uri': oauth_authorize_uri,
                        'instance.oauth.token_uri': oauth_token_uri}
            self.ays_install('openvstorage_oauth', 'openvcloud', instance='main', data=data_oauth)



class Cluster(AbstractConfig):
    """
    Cluster abstraction layer to allow for easier manipulation.
    """
    def __init__(self, config_path, development=False):
        super().__init__(config_path)
        self.k8s_config = '/root/.kube/{}.conf'.format(self.config['environment']['subdomain'])
        self.join_line = ''
        self.scripts_dir = '{}/kuberesources/'.format(REPO_PATH)
        self.development = development

    def install_kubernetes_cluster(self):
        """
        Will install a kubernetes master and minion nodes on the first and rest of the node list respectively.
        """
        for node in self.nodes:
            node.core.run('swapoff -a')

        managementips = list(get_controller_management_ips(self.config))
        k8s_config_data, self.join_line = self.prefab.virtualization.kubernetes.multihost_install(
                self.nodes, unsafe=True, reset=True, external_ips=managementips)
        self.prefab.core.dir_ensure('/root/.kube')
        self.prefab.core.file_write(self.k8s_config, k8s_config_data)
        if not self.prefab.core.file_exists('/root/.kube/config'):
            self.prefab.core.file_write('/root/.kube/config', k8s_config_data)

    def clone_repo(self, url=REPO_URL):
        for node in self.nodes:
            node.tools.git.pullRepo(url)

    def _install_kubectl(self, prefab):
        if not prefab.core.file_exists('/usr/local/bin/kubectl'):
            prefab.virtualization.kubernetes.install_kube_client(location='/usr/local/bin/kubectl')


    def install_controller(self):
        """
        Will use existing yaml or config scripts in this dir as well as jumpscale modules to install the controller setup on the
        cluster. Creating the relevant deployments, services, and mounts
        TODO
        """

        for node in self.nodes:
            self._install_kubectl(node)
        self._install_kubectl(self.prefab)
        self.clone_repo()

        directories = self.config['controller'].get('directories')
        for node in self.nodes:
            for directory in directories:
                node.core.dir_ensure(directory['path'], mode='777')

        copiedpaths = {}
        try:
            for key, value in self.config['environment']['ssl'].items():
                path = os.path.join(value['path'], '')
                if path not in copiedpaths:
                    tmpdir = self.first_node.core.file_get_tmp_path()
                    # make sure tmpdir has trailing slash
                    tmpdir = os.path.join(tmpdir, '')
                    self.first_node.core.dir_ensure(tmpdir)
                    self.first_node.core.upload(path, tmpdir)
                    copiedpaths[path] = tmpdir
                else:
                    tmpdir = copiedpaths[path]
                self.kubectl('create secret generic {name}-certs --from-file {dir}'.format(name=key, dir=tmpdir))
        finally:
            for tmpdir in copiedpaths.values():
                self.first_node.core.dir_remove(tmpdir)

        self.configure_keys()
        self.kub_client_apply()

    def apply_template(self, template, kind):
        self.kubectl('apply -f {template}'.format(template=template))
        template_name = j.sal.fs.getBaseName(template)
        timeout = time.time() + 240
        print('Waiting for %s to be ready..' % template_name)
        while time.time() < timeout:
            template_basename = template.split('/')[-1]
            template_name = template_basename.strip('.yaml') if template_basename.endswith('.yaml') else template_basename
            _, out, _ = self.kubectl('get %s %s -o json' % (kind, template_name), showout=False)
            json_out = j.data.serializer.json.loads(out)
            ready = False
            if kind == 'deployment':
                for condition in  json_out['status'].get('conditions'):
                    if condition['type'] == 'Available' and condition['status'] == 'True':
                        ready = True
                        break
            if kind == 'statefulset':
                if json_out['status']['replicas'] == json_out['status'].get('readyReplicas'):
                    ready = True
                    break
            if ready:
                break
        else:
            raise j.exceptions.Timeout('Deploying %s took longer than expected. Exiting..')
            

    def dump(self, location, data, dumper=j.data.serializer.yaml.dumps):
        data = dumper(data)
        self.first_node.core.file_write(location, data)

    def load(self, location, loader=j.data.serializer.yaml.loads):
        data = self.first_node.core.file_read(location)
        return loader(data)

    def write_service_template(self, template, externalips):
        template_loc = '{dir}{name}/{name}-service.yaml'.format(dir=self.scripts_dir, name=template)
        data = self.load(template_loc)
        if data['spec'].get('type') == 'NodePort':
            data['spec']['externalIPs'] = externalips
            self.dump(template_loc, data)
        return template_loc

    def upgrade(self):
        pubips = list(get_controller_public_ips(self.config))
        loc = self.write_service_template('upgrader', pubips)
        self.kubectl('apply -f {}'.format(loc))

    def prepare_templates(self):
        def write_deployment_template(template):
            template_loc = '{dir}{name}/{name}-deployment.yaml'.format(dir=self.scripts_dir, name=template)
            data = self.load(template_loc)
            for container in data['spec']['template']['spec']['containers']:
                volmounts = container.setdefault('volumeMounts', [])
                volmounts.append({'name': 'code', 'mountPath': '/opt/code/github/0-complexity', 'subPath': 'github/0-complexity'})
                volmounts.append({'name': 'code', 'mountPath': '/opt/code/github/jumpscale7', 'subPath': 'github/jumpscale7'})
            volumes = data['spec']['template']['spec'].setdefault('volumes', [])
            volumes.append({'name': 'code', 'hostPath': {'path': '/opt/code', 'type': 'Directory'}})
            self.dump(template_loc, data)

        self.first_node.core.upload('{}/scripts/kubernetes/'.format(REPO_PATH), self.scripts_dir)
        templates = ['agentcontroller', 'osis']
        externalips = list(get_controller_management_ips(self.config))
        for template in templates:
            self.write_service_template(template, externalips)

        pubtemplates = ['nginx', 'zero-access', 'management']
        pubips = list(get_controller_public_ips(self.config))
        for template in pubtemplates:
            self.write_service_template(template, pubips)

        if self.development:
            for template in ['osis', 'agentcontroller', 'portal']:
                write_deployment_template(template)
            for repo in DEV_REPOS:
                self.clone_repo(repo)

        stat_template = self.scripts_dir + '/stats-collector/stats-deployment.yaml'
        data = self.load(stat_template)
        data['spec']['template']['spec']['containers'][0]['args'][4] = self.config['network']['management']['network']
        self.dump(stat_template, data)

    def write_system_config(self):
        tmpdir = self.first_node.core.file_get_tmp_path()
        self.first_node.core.dir_ensure(tmpdir)
        tmpfile = j.sal.fs.joinPaths(tmpdir, 'system-config.yaml')
        try:
            self.dump(tmpfile, self.config)
            self.kubectl('create configmap system-config --from-file {path}'.format(
                            path=tmpfile))
        finally:
            self.first_node.core.dir_remove(tmpdir)

    def kube_client_apply(self):
        self.prepare_templates()
        self.write_system_config()

        self.kubectl('apply -f {path}/rbac.yaml'.format(path=self.scripts_dir))
        deployments = ['influxdb', 'osis', 'agentcontroller', 'stats-collector',
                       'portal', 'pxeboot', 'zero-access', 'management', 'controller-jsagent', 'nginx']
        statefulSets = ['mongo', 'syncthing']
        for statefulSet in statefulSets:
            template_file = self.scripts_dir + statefulSet
            self.apply_template(template_file, kind='statefulset')
        for deployment in deployments:
            template_file = self.scripts_dir + deployment
            self.apply_template(template_file, kind='deployment')
            
        self.grafana_apply('{}/grafana'.format(self.scripts_dir))

    def grafana_apply(self, grafana_dir):
        datasourcepath = '{}/provisioning/datasources/influx.yaml'.format(grafana_dir)
        dashboardpath = '{}/sources/templates'.format(grafana_dir)
        datasource = self.load(datasourcepath)
        datasourcename = 'controller_{}'.format(self.config['environment']['subdomain'])
        datasource['datasources'][0]['name'] = datasourcename
        self.dump(datasourcepath, datasource)
        for dashboardfile in self.first_node.core.find(dashboardpath, pattern='*.json'):
            db = self.load(dashboardfile, j.data.serializer.json.loads)
            db['id'] = None
            db['title'] += " ({})".format(self.config['environment']['subdomain'])
            for row in db['rows']:
                for panel in row['panels']:
                    panel['datasource'] = datasourcename
            if 'templating' in db:
                for item in db['templating']['list']:
                    item['datasource'] = datasourcename
            self.dump(dashboardfile, db, j.data.serializer.json.dumps)
        cmd = """
        cd {dir}
        kubectl create configmap grafana-provisioning-datasources --from-file=provisioning/datasources
        kubectl create configmap grafana-provisioning-dashboards --from-file=provisioning/dashboards
        kubectl create configmap grafana-dashboards --from-file=sources/templates
        kubectl apply -f grafana-service.yaml
        kubectl apply -f grafana-deployment.yaml
        """.format(config=self.k8s_config, dir=grafana_dir)
        self.first_node.core.execute_bash(cmd)

    def delete(self):
        cmd = """
        kubectl delete configmap --all --namespace default
        kubectl delete service --namespace default -l 'name != nginx'
        kubectl delete statefulset --all --namespace default
        kubectl delete deploy --all --namespace default
        """
        self.first_node.core.execute_bash(cmd)

    def update(self):
        print('Restarting cluster...')
        self.delete()
        self.kube_client_apply()

class PNode(Node):
    def ipmi(self, *args):
        ipmiip = get_ipaddress(self.node['ipmi']['ipaddress'])
        cmd = "ipmitool -I lanplus -H {} -U {} -P {} ".format(ipmiip, self.node['ipmi']['username'], self.node['ipmi']['password'])
        cmd += " ".join(args)
        self.first_node.core.run(cmd)

    def reboot(self):
        self.ipmi('chassis power cycle')

    def is_up(self):
        return j.sal.nettools.tcpPortConnectionTest(self.managementip, 22)

    def wait_up(self, timeout=300):
        j.sal.nettools.waitConnectionTest(self.managementip, 22, timeout)
        if not self.is_up():
            raise RuntimeError('Node did not come up in {}seconds'.format(timeout))

    def enable_pxe(self):
        prefab = self.get_node_from_appname('pxeboot')
        self.ipmi('chassis bootdev pxe')
        macaddress = str(netaddr.EUI(self.node['management']['macaddress'])).lower()
        dst = '/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}'.format(macaddress)
        src = '../../conf/tftp-911boot'
        prefab.core.file_link(src, dst)

    def disable_pxe(self):
        prefab = self.get_node_from_appname('pxeboot')
        macaddress = str(netaddr.EUI(self.node['management']['macaddress'])).lower()
        dst = '/var/ovc/pxeboot/tftpboot/pxelinux.cfg/01-{}'.format(macaddress)
        prefab.core.file_unlink(dst)

    def install_os(self):
        self.enable_pxe()
        self.reboot()
        time.sleep(20)
        self.wait_up()
        prefab = j.tools.prefab.getFromSSH(self.managementip)
        status, output, err = prefab.core.run('cd /root/tools/; ./Install')
        if status != 0:
            raise RuntimeError('Failed to install OS')
        self.disable_pxe()
        self.reboot()
        time.sleep(20)
        self.wait_up()
        self.update_keys()

###                    ###
# Command line interface #
###                    ###

@click.group()
@click.option('--config', help='Config file to deploy the cluster', envvar='ENV_CONFIG')
@click.pass_context
def cli(ctx, config):
    if not config:
        raise j.exceptions.Input('Please specify a config file')
    config = prepare_config(config)
    ctx.obj = config

@cli.group()
def cluster():
    pass

@cluster.group()
def resources():
    pass

@cluster.group()
def resource():
    pass

@cli.group()
def node():
    pass

@cli.group()
def cpu():
    pass

@cli.group()
def controller():
    pass

@cli.group()
def storage():
    pass

@cli.group()
def image():
    pass


@image.command('deploy')
@click.option('--name', help='Image name')
@click.pass_context
def image_deploy(ctx, name):
    storage = random.choice(ctx.obj['nodes']['storage'])
    jumpscale = JumpScale7(ctx.obj, storage['name'])
    jumpscale.ays_install(name, 'openvcloud')


@node.command('action')
@click.option('--name', help='Node name')
@click.argument('action')
@click.pass_context
def node_action(ctx, name, action):
    """
    Execute command for node
    """
    cluster = PNode(ctx.obj, name)
    getattr(cluster, action)()

@node.command('update')
@click.pass_context
def node_update(ctx):
    return _node_update(ctx)

def _node_update(ctx):
    """
    Update all pnodes
    """
    config = ctx.obj
    pnames = get_pnodes_names(config)
    pnode_action(pnames, 'update', config)
    pnode_action(pnames, 'restart', config)

@resources.command('writeconfig')
@click.pass_context
def write_system_config(ctx):
    """
    Write or update system-config in kubernetes configmap based on yaml file
    """
    cluster = Cluster(ctx.obj)
    cluster.write_system_config()


@resources.command('write')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def write_kube_resources(ctx, development):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj, development)
    cluster.prepare_templates()


@resources.command('apply')
@click.pass_context
def kube_apply(ctx):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj)
    cluster.kube_client_apply()
    
@resource.command('apply')
@click.option('--path', help='path to template')
@click.pass_context
def kube_apply(ctx, path):
    """
    Rewrite kube resources from template
    """
    cluster = Cluster(ctx.obj)
    cluster.apply_template(path, kind='deployment')

@cluster.command('deploy')
@click.option('--configure-cluster/--no-configure-cluster', help='Configure kubernetes cluster', default=True)
@click.pass_context
def deploy_cluster(ctx, configure_cluster):
    """
    Deploy will create the cluster machines and deploy kubernetes cluster on top of them.
    """
    cluster = Cluster(ctx.obj)
    if configure_cluster:
        cluster.install_kubernetes_cluster()
    cluster.install_controller()

@cluster.command('update')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def update_cluster(ctx, development):
    return _update_cluster(ctx, development)

def _update_cluster(ctx, development):
    """
    Will update the cluster.
    """
    cluster = Cluster(ctx.obj, development)
    cluster.update()

@cluster.command('upgrade')
@click.option('--development', default=False, is_flag=True)
@click.pass_context
def upgrade_cluster(ctx, development):
    """
    Will upgrade the env.
    """
    if not development:
        j.logger.set_level(30)
    config = ctx.obj
    pnames = get_pnodes_names(config)
    pnode_action(pnames, 'stop', config)
    pnode_action(pnames, 'update', config)
    cluster = Cluster(config, development)
    cluster.upgrade()
    cluster.update()
    pnode_action(pnames, 'start', config)
    print('Updating cluster done.')

@cpu.command('deploy')
@click.option('--name', help='Node name to configure')
@click.pass_context
def deploy_cpu(ctx, name):
    jumpscale = JumpScale7(ctx.obj, name)
    jumpscale.install_compute_node()


@controller.command('deploy')
@click.option('--name', help='Node name to configure')
@click.pass_context
def deploy_controller(ctx, name):
    jumpscale = JumpScale7(ctx.obj, name)
    jumpscale.install_controller()


@storage.command('deploy')
@click.option('--name', help='Node name to configure')
@click.pass_context
def deploy_storage(ctx, name):
    jumpscale = JumpScale7(ctx.obj, name)
    jumpscale.install_storage_node()





if __name__ == '__main__':
    cli()


